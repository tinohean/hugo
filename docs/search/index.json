[{"content":"数据湖概念性调研 近期数仓建设统一实时数据系统和离线报表无从着手，亟需进行架构上的升级。以下是针对传统数仓的痛点而产生的数据湖概念性调研，也包括一些大数据基本概念的阐述。\n数据仓库   数据库(Database)\n传统的关系型数据库的主要应用是联机事务处理OLTP（On-Line Transaction Processing），主要是基本的、日常的事务处理，例如业务报表。 常见的Mysql,Oracle,MariaDB等。\n  数据仓库(Datawarehouse)\n数据仓库系统的主要应用主要是OLAP（On-Line Analytical Processing），支持复杂的分析操作，侧重决策支持，并且提供直观易懂的查询结果。 常见的Hive，Druid等。\n  E.F.Codd提出了关于OLAP的12条准则：\n OLAP模型必须提供多维概念视图 透明性准则 存取能力准则 稳定的报表能力 客户/服务器体系结构 维的等同性准则 动态的稀疏矩阵处理准则 多用户支持能力准则 非受限的跨维操作 直观的数据操纵 灵活的报表生成 不受限的维与聚集层次  数据分层 数据分层的目的\n 清晰数据结构\n每一个数据分层都有它的作用域，这样我们在使用表的时候能更方便地定位和理解。 数据血缘追踪\n简单来讲可以这样理解，我们最终给业务呈现的是一张能直接使用的业务表，但是它的来源有很多，如果有一张来源表出问题了，我们希望能够快速准确地定位到问题，并清楚它的危害范围。 减少重复开发\n规范数据分层，开发一些通用的中间层数据，能够减少极大的重复计算。 把复杂问题简单化\n把一个复杂的任务分解成多个步骤来完成，每一层只处理单一的步骤，比较简单和容易理解。而且便于维护数据的准确性，当数据出现问题之后，可以不用修复所有的数据，只需要从有问题的步骤开始修复。 屏蔽原始数据的异常\n屏蔽业务的影响，不必改一次业务就需要重新接入数据。  收益   清晰数据结构 每一个数据分层都有它的作用域和职责，在使用表的时候能更方便地定位和理解\n  减少重复开发 规范数据分层，开发一些通用的中间层数据，能够减少极大的重复计算\n  统一数据口径\n通过数据分层，提供统一的数据出口，统一对外输出的数据口径\n  复杂问题简单化\n将一个复杂的任务分解成多个步骤来完成，每一层解决特定的问题\n  分层设计 [ODS] -\u0026gt; [DW] -\u0026gt; [APP]  数据运营层（ ODS ）\n存放接入的原始数据。 数据仓库层（DW）\n存放要重点设计的数据仓库中间层数据。 数据应用层（APP）\n面向业务定制的应用数据  数据运营层ODS 是最接近数据源中数据的一层，数据源中的数据，经过抽取、洗净、传输，也就说传说中的 ETL 之后，装入本层。本层的数据，总体上大多是按照源头业务系统的分类方式而分类的。\n一般来讲，为了考虑后续可能需要追溯数据问题，因此对于这一层就不建议做过多的数据清洗工作，原封不动地接入原始数据即可，至于数据的去噪、去重、异常值处理等过程可以放在后面的DWD层来做。\n数据仓库层DW 数据仓库层是我们在做数据仓库时要核心设计的一层，在这里，从 ODS 层中获得的数据按照主题建立各种数据模型。 DW层又细分为 DWD（Data Warehouse Detail）层、DWM（Data WareHouse Middle）层和DWS（Data WareHouse Service）层。\n 数据明细层：DWD（Data Warehouse Detail）  该层一般保持和ODS层一样的数据粒度，并且提供一定的数据质量保证。同时，为了提高数据明细层的易用性，该层会采用一些维度退化手法，将维度退化至事实表中，减少事实表和维表的关联。\n另外，在该层也会做一部分的数据聚合，将相同主题的数据汇集到一张表中，提高数据的可用性。\n数据中间层：DWM（Data WareHouse Middle）  该层会在DWD层的数据基础上，对数据做轻度的聚合操作，生成一系列的中间表，提升公共指标的复用性，减少重复加工。\n直观来讲，就是对通用的核心维度进行聚合操作，算出相应的统计指标。\n数据服务层：DWS（Data WareHouse Service）  又称数据集市或宽表。按照业务划分，如流量、订单、用户等，生成字段比较多的宽表，用于提供后续的业务查询，OLAP分析，数据分发等。\n一般来讲，该层的数据表会相对比较少，一张表会涵盖比较多的业务内容，由于其字段较多，因此一般也会称该层的表为宽表。\n在实际计算中，如果直接从DWD或者ODS计算出宽表的统计指标，会存在计算量太大并且维度太少的问题，因此一般的做法是，在DWM层先计算出多个小的中间表，然后再拼接成一张DWS的宽表。由于宽和窄的界限不易界定，也可以去掉DWM这一层，只留DWS层，将所有的数据在放在DWS亦可。\n数据应用层 主要是提供给数据产品和数据分析使用的数据，一般会存放在 ES、PostgreSql、Redis等系统中供线上系统使用，也可能会存在 Hive 或者 Druid 中供数据分析和数据挖掘使用。比如我们经常说的业务数据，一般就放在这里。\n 传统数仓痛点 传统数仓一般落地在HDFS中，这会导致一些问题\n 处理成本大\n批量导入到文件系统的数据一般都缺乏全局的严格schema规范，下游的Spark作业做分析时碰到格式混乱的数据会很麻烦，每一个分析作业都要过滤处理错乱缺失的数据，成本较大。 读取不友好\n数据写入文件系统这个过程没有ACID保证，用户可能读到导入中间状态的数据。所以上层的批处理作业为了躲开这个坑，只能调度避开数据导入时间段，可以想象这对业务方是多么不友好；同时也无法保证多次导入的快照版本，例如业务方想读最近5次导入的数据版本，其实是做不到的。 无法高效更新/删除历史数据\nparquet文件一旦写入HDFS文件，要想改数据，就只能全量重新写一份的数据，成本很高。事实上，这种需求是广泛存在的，例如由于程序问题，导致错误地写入一些数据到文件系统，现在业务方想要把这些数据纠正过来；线上的MySQL binlog不断地导入update/delete增量更新到下游数据湖中；某些数据审查规范要求做强制数据删除，例如欧洲出台的GDPR隐私保护等等。 文件系统负载大\n频繁地数据导入会在文件系统上产生大量的小文件，导致文件系统不堪重负，尤其是HDFS这种对文件数有限制的文件系统。 回溯成本高\n多份全量存储带来的存储浪费，数仓设计中为了保证用户可以访问数据某个时间段的历史状态，会将全量数据按照更新日期留存多份，故大量未变化的历史冷数据会被重复存储多份，带来存储浪费。 调度启动晚\n大规模的数据落地HDFS后，只能在凌晨分区归档后才能查询并做下一步处理； 数据量较大的RDS数据同步，需要在凌晨分区归档后才能处理，并且需要做排序、去重以及 join 前一天分区的数据，才能产生出当天的数据。 更新模式重\n存在较多数据的冗余更新增量数据的分布存在长尾形态，故每日数仓更新需要加载全量历史数据来做增量数据的整合更新，整个更新过程存在大量历史数据的冗余读取与重写，带来的过多的成本浪费，同时影响了更新效率。 重复读取多\n仅能通过分区粒度读取数据，在分流等场景下会出现大量的冗余 IO。  数据湖 概念 数据湖（Data Lake）是可以存储大量结构化、半结构化和非结构化数据的存储仓库。它是一个以其原生格式存储每种类型数据的场所，对帐户大小或文件没有固定限制。它提供大量数据，以提高分析性能和原生集成。\n数据湖就像一个大容器，与真实的湖泊和河流非常相似。就像在湖中有多个支流进入一样，数据湖具有结构化数据，非结构化数据，机器对机器，实时流经的日志。\n数据湖使数据更易处理，是一种经济高效的方式来存储组织的所有数据以供以后处理。数据分析师可以专注于发现数据中的有意义的模式，而不是数据本身。\n与将数据存储在文件和文件夹中的分层数据仓库不同，Data lake具有扁平的体系结构。数据湖中的每个数据元素都有一个唯一的标识符，并用一组元数据信息进行标记。\n数据湖VS数据仓库    参数 数据湖 数据仓库     数据 存储任何内容 仅关注业务流程   处理 大部分未经处理 高度处理   数据类型 非结构化，半结构化，结构化 表格形式和结构   任务 共享数据管理 针对数据检索进行了优化   敏捷 高度敏捷，根据需要配置和重新配置 较之不敏捷，配置固定   用户 数据科学家 业务专业人员   存储 低成本存储 快速响应的昂贵存储   EDW更换 可以作为EDW来源 与EDW互补（不可替代）   模式 读时模式（无预定义） 写时模式（预定义）   数据处理 帮助快速提取新数据 耗时   数据粒度 粗 细   工具 Hadoop/ Map Reduce 主要商业工具    架构 示例架构  架构 \n方案选型 存储引擎开源方案主要包括Hudi,Iceberg,Delta三大产品。\n    Hudi Iceberg Delta     引擎支持 Spark、Flink Spark、Flink Spark   原子语义 Delete/Update/Merge Insert/Merge Delete/Update/Merge   流式写入 支持 支持 支持   文件格式 Avro、Parquet、ORC Avro、Parquet、ORC Parquet   MOR能力 支持 不支持 不支持   Schema Evolution 支持 支持 支持   Cleanup能力 自动 手动 手动   Compaction 自动/手动 手动 手动   小文件管理 自动 手动 手动    Delta的房子底座相对结实，功能楼层也建得相对比较高，但这个房子其实可以说是databricks的，本质上是为了更好地壮大Spark生态， 在delta上其他的计算引擎难以替换Spark的位置，尤其是写入路径层面。\nIceberg的建筑基础非常扎实，扩展到新的计算引擎或者文件系统都非常的方便， 但是现在功能楼层相对低一点，目前最缺的功能就是upsert和compaction两个，Iceberg社区正在以最高优先级推动这两个功能的实现。\nHudi的情况要相对不一样，它的建筑基础设计不如iceberg结实， 举个例子，如果要接入Flink作为Sink的话，需要把整个房子从底向上翻一遍，把接口抽象出来，同时还要考虑不影响其他功能， 当然Hudi的功能楼层还是比较完善的，提供的upsert和compaction功能直接命中广大群众的痛点。\n期待架构  平台支持流批一体，统一实时与离线逻辑； 利用Flink + Hudi技术栈搭建实时数仓，构建kafka -\u0026gt; ods -\u0026gt; dwd -\u0026gt; olap的实时数据链条，满足业务近实时需求 利用hive构建数据分析系统，提供前端界面，实现低门槛数据分析  收益与风险 使用数据湖的主要好处如下：\n 充分帮助产品化和高级分析 提供经济高效的可扩展性和灵活性 从无限的数据类型中提供价值 降低长期持有成本 允许经济的存储文件 快速的适应变化 数据湖的主要优势是不同内容源的集中化 各个部门的用户可能遍布全球，可以灵活的访问数据  使用数据湖的风险如下：\n 一段时间后，数据湖可能会失去相关性和动力 数据湖的设计风险较大 非结构化数据可能导致无法掌控的混乱、不可用的数据、不同且复杂的工具、企业范围的写作、统一、一致和通用问题 增加了存储和计算的成本 无法从数据打交道的其他与人那里得到见解 数据湖最大的风险是安全和访问控制。有时，数据在没有任何监督的情况下放入湖中，某些些数据具有隐私和监管要求  参考文档  https://www.cnblogs.com/itboys/p/10592871.html https://segmentfault.com/a/1190000020385389?utm_source=sf-related https://mp.weixin.qq.com/s?__biz=MzkwOTIxNDQ3OA==\u0026mid=2247532544\u0026idx=1\u0026sn=86b24d40316f6b8d0be6c82606bcfb7f\u0026source=41#wechat_redirect https://mp.weixin.qq.com/s/2WUZhMnNfY4mzEng9TZ2Uw https://segmentfault.com/a/1190000040715962 https://zhuanlan.zhihu.com/p/346546088  ","date":"2021-10-15T13:36:46+08:00","image":"http://tinohean.top/p/%E6%95%B0%E6%8D%AE%E6%B9%96%E5%8F%8A%E5%85%B6%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5%E6%80%A7%E8%B0%83%E7%A0%94/lake_hu4a54f6cc00d4e0599bb1fb51fbd2eda0_63589_120x120_fill_q75_box_smart1.jpeg","permalink":"http://tinohean.top/p/%E6%95%B0%E6%8D%AE%E6%B9%96%E5%8F%8A%E5%85%B6%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5%E6%80%A7%E8%B0%83%E7%A0%94/","title":"数据湖及其相关概念性调研"},{"content":"部署策略 不同的应用有不同的部署场景，有的应用必须是滚动更新的，不可以有服务停止的时间。而有些应用由于某些原因，比如不支持新旧版本共存，必须是版本一键切换的。\n常见的部署策略包括蓝绿发布，金丝雀发布（灰度发布），A/B Test发布等。\n基础发布 基础发布是最为原始也是最为简单的一种发布策略，即替换式更新。把线上所有服务一次性替换为更新版本。缺点是回滚慢，风险较高，服务有中断时间。  基础发布 \n多服务发布 服务有多版本共存的情况下，可以采取跟基础发布类似的策略，同时一次性更新。但由于多版本共存，因此风险比基础发布更低一些，但仍然存在某个版本出问题的可能性。缺点是回滚慢，多版本带来的管理复杂，测试复杂等。  多服务发布 \n滚动发布 滚动发布是滚动式更新服务的每一个节点。好处是回滚相对简单，滚动的方式更容易发现问题，因此风险较低。不过，服务必须支持新旧版本共存的情况。  滚动发布 \n蓝绿发布 蓝绿发布是环境镜像替换。绿环境是线上运行环境，蓝环境是需要发布版本的另一个环境，把流量从绿环境切换到蓝环境完成版本更新。\n蓝环境一般会进行深度测试，因此风险较低。此外，流量切换很快捷，回滚是瞬间完成的。缺点是成本较高，在一些复杂的微服务场景下也很难做到覆盖所有的回归测试，从而可能造成服务中断或其他问题。  蓝绿发布 \n金丝雀发布 金丝雀发布（国内一般称之为灰度发布）类似于滚动发布，以增量的方式进行版本迭代。不同的是金丝雀发布更倾向于百分比的推进，并可以指定发布的节点，从而可以在增量迭代的过程中进行数据监控及比对，从而提早发现问题。\n好处是风险低，成本比蓝绿发布低，回滚方便。缺点也是显而易见的，比对和测试需要更长的时间周期和更复杂的运维操作。不停迭代的新版本数量也需要监控的更新。\n 金丝雀发布 \nA/B测试发布 A/B Test需要多个版本在线上同时运行进行指标测量和比较。可以通过路由规则，A/B测试工具等实现流量分流及测量。最后根据比较结果选择最好的版本进行全量发布。\nA/B测试倾向于实验和探索测试。优点是成本低，工具齐全。缺点是存在风险，测试复杂，自动化实现更困难。\n A/B测试发布 \n部署策略的选择 一般来说，蓝绿发布和金丝雀发布是比较常用的发布策略。如果应用支持新旧版本共存的话，首选是金丝雀发布，如果不支持的话，首选是蓝绿发布。\n对于一些故障容忍度较高的服务，可以采用最为简单的基础发布策略。\n基于风险和成本简单归纳一下\n低风险，低成本： 金丝雀发布\n高风险，低成本： 基础发布\n低风险，高成本： 蓝绿发布\n 部署策略的容器化实现 基础发布 更新Deployment的.spec.strategy.type==Recreate\n则默认部署策略会变成重新创建，也就是基础发布的策略方式：杀死所有Pod,然后重新创建新版本的Pod\n滚动发布 kubernetes的Deployment默认部署策略就是滚动发布，好处是通过探针可以实时检测服务的健康状况，在新Pod就绪健康之前会一直等待，从而避免更大规模的故障风险。\n控制发布速度  minReadySeconds\n这个属性指定新创建的Pod在运行多少秒后才能将其视为可用。对于一些需要加载数据到内存的应用来说尤为有用。 maxSurge\n这个属性决定了Deployment中除了配置的期待副本数量外，最多允许超出的pod实例数量。也决定了升级的速度，数量越大，升级更新越快。 maxUnavailable\n设置在滚动发布期间，Deployment最大允许多少个pod处于不可用状态。谨慎设置，尤其是在流量高峰期。一般我们设置为0，副本数量会一直保持在期待状态中。  配置示例 apiVersion:apps/v1kind:Deploymentmetadata:creationTimestamp:nulllabels:app:nginxname:nginxspec:replicas:10minReadySeconds:10selector:matchLabels:app:nginxstrategy:rollingUpdate:maxSurge:1maxUnavailable:1type:RollingUpdatetemplate:metadata:creationTimestamp:nulllabels:app:nginxspec:containers:- image:nginxname:nginxresources:{}readinessProbe:failureThreshold:10httpGet:path:/readyport:80scheme:HTTPinitialDelaySeconds:5periodSeconds:5successThreshold:1timeoutSeconds:3status:{}金丝雀发布 尽管可以通过kubectl rollout pause暂停滚动更新方式实现手动粗粒度灰度，但由于无法精准控制比例，官方文档建议是建立多个Deployment实现。\n具体思路  建立多个Deployments,通过标签识别是否是灰度版本，比如release: canary和release: stable。 前端忽略上述标签，统一流量入口。 通过不断调整canary Deployment和线上版本的副本数，来达到递增更新的目的。  配置示例 name:frontendreplicas:3...labels:app:guestbooktier:frontendtrack:stable...image:gb-frontend:v3---name:frontend-canaryreplicas:1...labels:app:guestbooktier:frontendtrack:canary...image:gb-frontend:v4---selector:app:guestbooktier:frontend自动调整副本数 难点是需要手动调整两个Deployment的副本数，如果使用HPA的话只需要调整一个Deployment就好了,另外一个Deployment会根据压力自动伸缩。\n一个比较讨巧的办法是创建多个Deployment,比如10%流量比例的，30%流量比例的，60%流量比例的，然后串行发布即可。\n自动化实现副本数的调整比较复杂，好在有开源案例可以实现，比如Flagger。  Flagger \n原理就是定期分析canary的请求成功率和延迟，逐渐增大canary的流量权重，直至全量更新。\n除了金丝雀发布外，Flagger还支持A/B Test发布策略。\n蓝绿发布 蓝绿发布的实现较为简单，只需要创建一个green Deployment,然后通过标签选择器，把前端SVC由blue Deployment指向green Deployment即可。\n配置示例 apiVersion:apps/v1kind:Deploymentmetadata:creationTimestamp:nulllabels:release:bluename:nginxspec:replicas:1selector:matchLabels:app:nginxstrategy:{}template:metadata:creationTimestamp:nulllabels:app:nginxspec:containers:- image:nginxname:nginxresources:{}status:{}---apiVersion:apps/v1kind:Deploymentmetadata:creationTimestamp:nulllabels:release:greenname:nginxspec:replicas:1selector:matchLabels:app:nginxstrategy:{}template:metadata:creationTimestamp:nulllabels:app:nginxspec:containers:- image:nginxname:nginxresources:{}status:{}---apiVersion:v1kind:Servicemetadata:name:nginxlabels:name:nginxspec:ports:- name:httpport:80targetPort:80selector:release:greentype:LoadBalancer参考文档  https://harness.io/blog/continuous-verification/blue-green-canary-deployment-strategies/ https://medium.com/tech-at-wildlife-studios/canary-deployment-in-kubernetes-how-to-use-the-pattern-b2e9c40d085d https://kubernetes.io/zh/docs/concepts/cluster-administration/manage-deployment/ https://www.ianlewis.org/en/bluegreen-deployments-kubernetes https://docs.flagger.app/tutorials/nginx-progressive-delivery  ","date":"2021-09-21T23:11:56+08:00","image":"http://tinohean.top/p/%E9%83%A8%E7%BD%B2%E7%AD%96%E7%95%A5%E5%8F%8A%E5%AE%B9%E5%99%A8%E5%8C%96%E5%AE%9E%E7%8E%B0/canary_hu027b0aea857f9d892a6eb44041b99305_1876819_120x120_fill_q75_box_smart1.jpeg","permalink":"http://tinohean.top/p/%E9%83%A8%E7%BD%B2%E7%AD%96%E7%95%A5%E5%8F%8A%E5%AE%B9%E5%99%A8%E5%8C%96%E5%AE%9E%E7%8E%B0/","title":"部署策略及容器化实现"},{"content":"浅谈ChatOps 背景 本文源自我在技术团队的一次分享。 八月的时候我调研了下ChatOps概念，并通过钉钉机器人成功落地。\n至于当时为什么要做ChatOps呢，主要是因为实力不足（不懂算法，监控数据量有限），无法推进AIOps进行运维平台的迭代升级。而ChatOps作为一个新兴的概念（其实也不新了），也属于智能运维的范畴，在国内还没有什么很红的应用方案。\n其次是当时出现了一些日常操作的痛点，比如某次研发小伙伴在午饭前上了一次线，午饭期间出现重大故障，只能捧着饭碗从食堂火速跑回工位回滚处理，再比如临睡前懒得去登陆监控页面的我想看下当前负载较大的服务器状况\u0026hellip;\n在这样的契机下，经过一系列脑洞挖掘出一些潜在的用途后，一款python+dingtalk的简单原型就出来了。部署简单，实用，可定制性强。\n概念 理解什么是ChatOps，只需要了解两个要点\n 会话驱动 机器执行  也就是说，这个系统是通过对话式的聊天进行驱动的，而背后真正的执行者从人变成了机器人。\n概念最早源于Github的Hubot项目，开发者可以通过在pull request页面@机器人进行一些常规的分支合并等操作。\n比如当下火热的kubernetes代码库， 一个叫k8s-ci-robot的机器人频频出现在开发者的视线。当然它在ChatOps的实践上已经走得很成熟了，可以胜任的一些常规操作包括：提醒代码库管理者review期限, 自动测试，自动加标签，自动格式检查，自动关闭长时间的issue,随机指定管理人员进行代码review等等。\n这在很大程度上减轻了项目的管理工作负担，从简化的流程上间接提高了效率。\n优点 了解了什么是ChatOps后，我们可以总结下所谓的ChatOps能给我们带来什么？\n概括下来，ChatOps大概有以下几个收益：\n 公开透明 上下文共享 移动高效  第一个，显而易见的，所有操作都是在聊天平台中公开的，有聊天记录可以查询，所有人都能看到彼此的操作记录，由此带来的第二个好处，就是上下文共享，一些繁琐复杂的操作流程，可以通过查看记录了解进度，协调，这样工作承接会更加有序。\n还有一个很明显的好处就是移动高效，因为我们的操作端从电脑转移到了手机，pad等移动设备，也不需要输入冗长的命令，做很多次点击等操作，实际上会提高处理效率。\n使用场景 脑洞一番后，总结了一些可使用的场景。\n第一个，运维巡检，比如哪个夜黑风高的晚上，临睡前的我预感不妙，要看下现在硬件容量有可能爆满的服务器详情，这样可以提前处理下。\n另一个，就是告警查询，有时候告警泛滥，我如果不登陆监控页面的话，不清楚哪些告警已经处理了，哪些没有处理。这时候我可以@机器人来一个当前告警清单。\n第三个呢，就是紧急回滚。如同前文所说，比如哪天我们上好线，初期观察没啥问题，出去吃饭了，结果吃饭过程中出现大量告警，直接@机器人回滚操作，简单又高效。\n第四个呢，就是我们经常需要做的，硬件信息查询，比如服务器外网IP，硬件配置，比登陆CMDB查询方便多了。\n\u0026hellip;\n架构  ChatOps系统架构  一个完整的ChatOps落地包括这四个层面的架构，有人（不限于运维，研发，甚至可以是运营），有聊天平台（比如国内的钉钉，微信），有机器人（比如github的hubot），有支持机器人操作的基础设施，包括服务器，脚本，后端服务等等。\n后端服务作为ChatOps的主要开发对象，暂时没有很契合的开源方案，需要开发者自己开发相关逻辑。\n等级演变 下面分析下ChatOps的演变路线。  before ChatOps \n在ChatOps之前，运维是直接对接监控系统的，没有聊天软件来进行信息同步。同时，告警处理和事件处理等运维操作都是基于Ad-hoc命令，通过shell客户端进行的。  lv1 \n到了Level1后呢，我们会针对告警创建不同的讨论群，比如某应用触发告警了，运维在群里通知开发进行处理。在这个层级，事件的发送者还是人。  lv2a \nLevel 2A后，我们已经可以把这些告警信息、事件详情等进行分类通知，比如硬件类告警专门发到一个群，应用类告警发到另一个群。这时候，事件的发送者变成了监控系统，然后还支持事件恢复通知。  lv2b \nLevel 2B支持从监控系统中拉取数据发送到聊天平台了，比如查询CMDB获取硬件信息，工单系统啊，监控指标啊等信息。此时信息更加丰富了。  lv3 \n到了level3后就是全自动化的交互。比如可以更新工单，事件状态，发出指令并查看执行结果。最重要的是我们能通过聊天工具跟监控系统等内部系统进行交互。  lv4 \n到了level4后，交互的中间人变成了机器人，机器人可以把对话转发到工单系统中，然后监控关键字并发送更新信息，更新知识库，总之就是交互更加智能。  lv5 \n到了level5后。机器人演变成人工智能了，对于自然语言的理解有很大的提高，并可以根据知识库和历史记录推荐具体的解决方案了。此时可以认为达到了AIOps的层级了。\n方案选型 当今市面上开源机器人主要包括这三家\n Hubot Lita Err  第一个就是我们前面所说的github开发的Hubot, 而Lita是一个ruby版本的Hubot,使用redis进行数据持久化， ErrBot是基于python开发的，内部支持rocketchat这个开源的聊天平台。\n这几个选型的缺点都是对国内聊天软件的支持度不够，不符合我们的使用习惯，另外就是hubot已经超过两年没有更新了，Errbot也是，rocketchat插件年久失修，新版本都不再支持。\n而钉钉集成的机器人，使用outgoing功能接受信息并进行处理，集成在当前的监控体系下，是一个最好的选择。\n实现逻辑 首先，我们要先开发一个后端服务，作为和钉钉机器人的交互对象。具体接口功能包括\n 接收钉钉机器人的GET请求，作为健康检查。返回200 接收钉钉机器人转发的各种Post请求，并进行处理 回传处理结果给钉钉机器人  然后，通过钉钉开发者后台创建一个内部应用，填入后端服务器的链接(此处GET请求属于健康检查，需要正常返回200即可)。\n最后，创建一个内部群，添加内部应用的机器人，将机器人token配置为后端服务的发送token。\n要点：\n 钉钉机器人有qps上限（20次/min),此处可以借助queue模块实现频次控制。 示例代码如下：  que = queue.Queue(20) while True: conn, address = sock.accept() now = time.time() que.put(now) if que.full(): elapse_time = now - que.get() if elapse_time \u0026lt; 60: sleep_time = int(60 - elapse_time) + 1 logging.warn(\u0026#39;当前发送频率已达限制每分钟20次上线，将等待 {}s\u0026#39;.format(str(sleep_time))) time.sleep(sleep_time) p = Process(target=handle_request, args=(conn,)) p.start() conn.close()  对于请求的body解析要考虑交互的不同输入情况，比如大小写匹配，空格过滤等。 做好Exception处理并进行提示。比如频次上限后的错误信息。 有默认帮助信息，便于使用（此处我设置空白输入和help两种模式触发）。 敏感token和密钥等通过configparser模块与代码分离。 示例代码如下：  def calc_sign(timestamp): app_secret = config[\u0026#39;dingtalk\u0026#39;][\u0026#39;app_secret\u0026#39;] app_secret_enc = app_secret.encode(\u0026#39;utf-8\u0026#39;) string_to_sign = \u0026#39;{}\\n{}\u0026#39;.format(str(timestamp), app_secret) string_to_sign_enc = string_to_sign.encode(\u0026#39;utf-8\u0026#39;) hmac_code = hmac.new(app_secret_enc, string_to_sign_enc, digestmod=hashlib.sha256).digest() sign = base64.b64encode(hmac_code).decode(\u0026#39;utf-8\u0026#39;) return sign  请求body的senderStaffId对应webhook的atUserIds，markdown格式需要在text处也写入@信息。\n示例代码如下：  def send_markdown(post_userid, send_msg): message = { \u0026#34;msgtype\u0026#34;: \u0026#34;markdown\u0026#34;, \u0026#34;markdown\u0026#34;: { \u0026#34;title\u0026#34;:\u0026#34;运维机器人\u0026#34;, \u0026#34;text\u0026#34;: \u0026#39;@\u0026#39; + post_userid + \u0026#39;\\n\u0026#39; + send_msg }, \u0026#34;at\u0026#34;: { \u0026#34;atUserIds\u0026#34;: [ post_userid ], \u0026#34;isAtAll\u0026#34;: False } } return json.dumps(message) 参考文档  https://www.jianshu.com/p/7aa2ced21302 https://www.ibm.com/garage/method/practices/manage/chatops/ https://github.com/zhuifengshen/DingtalkChatbot/blob/master/dingtalkchatbot/chatbot.py https://developers.dingtalk.com/document/robots/enterprise-created-chatbot?spm=ding_open_doc.document.0.0.34f16573JlceHR#topic-2097982 https://community.ibm.com/HigherLogic/System/DownloadDocumentFile.ashx?DocumentFileKey=f6d1c85a-f2b2-9061-64e4-17be48ed3756\u0026forceDialog=0  ","date":"2021-09-02T23:51:08+08:00","image":"http://tinohean.top/p/%E6%B5%85%E8%B0%88chatops/chatops_hu2af93606f657e048f3b8c35cecf8603a_39849_120x120_fill_q75_box_smart1.jpeg","permalink":"http://tinohean.top/p/%E6%B5%85%E8%B0%88chatops/","title":"浅谈ChatOps"},{"content":"Paddle飞桨部署文档 Paddle是百度开源的一款深度学习平台\n基础环境 环境说明    项 版本     GPU NVIDA Tesla P4   操作系统 ubuntu 18.04 x86_64   硬件规格 4C20G 1Mbps + 100GB   Python 3.7.10   包管理 Anaconda 3   telsa驱动 NVIDIA-Linux-x86_64-418.152.00   cuda驱动 cuda_10.1.243_418.87.00_linux    环境预设  检查显卡  lspci | grep -i nvidia 2.确认gcc版本,没有则安装\ngcc --version 3.安装内核开发包\nsudo apt-get install linux-headers-$(uname -r) 4.禁掉Nouveau驱动\necho \u0026#39;\u0026#39;\u0026#39;blacklist nouveau options nouveau modeset=0\u0026#39;\u0026#39;\u0026#39; | sudo tee /etc/modprobe.d/blacklist-nouveau.conf sudo update-initramfs -u 5.重启，检查模块是否成功禁用\nsudo reboot # 检查无输出则正常 lsmod |grep nouveau 安装 tesla驱动   安装dkms\nsudo apt-get install dkms   下载驱动\nhttp://www.nvidia.com/Download/Find.aspx\n   类别 选项     Product Type Data Center / Telsa   Product Series P-Series   Product Telsa P4   Operating System Linux 64-bit   CUDA Toolkit 10.1   Language English(US)    选择Telsa Driver for Linux 64 # 418.152.00进行下载\n  安装\nchmod +x NVIDIA-Linux-x86_64-418.152.00.run sudo ./NVIDIA-Linux-x86_64-418.152.00.run   验证\nnvidia-smi 输出如下\n+-----------------------------------------------------------------------------+ | NVIDIA-SMI 418.152.00 Driver Version: 418.152.00 CUDA Version: 10.1 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | |===============================+======================+======================| | 0 Tesla P4 Off | 00000000:00:09.0 Off | 0 | | N/A 24C P8 6W / 75W | 0MiB / 7611MiB | 0% Default | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: GPU Memory | | GPU PID Type Process name Usage | |=============================================================================| | No running processes found | +-----------------------------------------------------------------------------+   安装 cuda驱动 https://cloud.tencent.com/document/product/560/8064\n1.下载驱动包\nwget https://developer.download.nvidia.com/compute/cuda/10.1/Prod/local_installers/cuda_10.1.243_418.87.00_linux.run chmod +x cuda_10.1.243_418.87.00_linux.run sudo bash cuda_10.1.243_418.87.00_linux.run --toolkit --samples --silent 配置环境变量  echo \u0026#39;\u0026#39;\u0026#39;export PATH=/usr/local/cuda/bin:$PATH export LD_LIBRARY_PATH=/usr/local/cuda-10.1/lib64${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}\u0026#39;\u0026#39;\u0026#39; | sudo tee /etc/profile.d/cuda.sh source /etc/profile 重启 验证安装结果  cd /usr/local/cuda-10.1/samples/1_Utilities/deviceQuery # 报错g++ not found 则按需安装g++ sudo make ./deviceQuery 返回Result=PASS 则表示安装成功\n安装 anaconda   下载安装脚本\nhttps://www.anaconda.com/products/individual\nwget https://repo.anaconda.com/archive/Anaconda3-2020.11-Linux-x86_64.sh   执行安装\n  bash Anaconda3-2020.11-Linux-x86_64.sh 安装后重登陆加载环境变量\n 更新国内源\nconda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/ conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/ conda config --set show_channel_urls yes   Paddle部署 https://www.paddlepaddle.org.cn/install/quick?docurl=/documentation/docs/zh/2.0/install/conda/linux-conda.html\n创建虚拟环境 conda create -n paddle_env python=3.7 conda activate paddle_env 安装 paddle conda install paddlepaddle-gpu==2.0.0 cudatoolkit=10.1 --channel https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/Paddle/ 其他依赖库安装 # paddlehub不支持conda安装 # https://github.com/PaddlePaddle/PaddleHub/issues/499 pip install paddlehub conda install shapely conda install pyclipper 验证安装 $ python \u0026gt;\u0026gt;\u0026gt; import paddle \u0026gt;\u0026gt;\u0026gt; paddle.utils.run_check() 输出installed successfully则表示安装成功\nPaddlePaddle works well on 1 GPU. PaddlePaddle works well on 1 GPUs. PaddlePaddle is installed successfully! Let's start deep learning with PaddlePaddle now. ","date":"2021-03-02T13:33:54+08:00","image":"http://tinohean.top/p/paddle%E9%A3%9E%E6%A1%A8%E9%83%A8%E7%BD%B2%E6%96%B9%E6%A1%88/paddle_hu5438825b9b6d1014226d20d231e650c2_191045_120x120_fill_q75_box_smart1.jpeg","permalink":"http://tinohean.top/p/paddle%E9%A3%9E%E6%A1%A8%E9%83%A8%E7%BD%B2%E6%96%B9%E6%A1%88/","title":"Paddle飞桨部署方案"},{"content":" 摄于巴厘岛海神庙 2021年1月13日 下午3:30 \n","date":"2021-01-13T15:30:08+08:00","image":"http://tinohean.top/p/%E6%91%84%E5%BD%B1-%E6%B5%B7%E7%A5%9E%E5%BA%99/bali_hub4e16b60b974949ec4c4803197c3b3b2_3400421_120x120_fill_q75_box_smart1.jpg","permalink":"http://tinohean.top/p/%E6%91%84%E5%BD%B1-%E6%B5%B7%E7%A5%9E%E5%BA%99/","title":"【摄影】 海神庙"},{"content":"Soul 早早在床上喝奶，她的口头禅变成了巴布，也就是抱抱的意思。\n这是她会说的为数不多的两个字的话。在她的世界里，什么都可以巴布。奶瓶，玩偶，鞋鞋，袜袜……她对她所有爱的事物说巴布，抱着的同时还会学着大人抱她那样摇晃。\n看着很幼稚，却觉得格外美好。\n每一个眼中的事物都可以成为她的spark。如同22号灵魂在地球上一样。\n今年的最后一天，我看完了soul。年度最佳影片终究姗姗来迟。\n什么是人生的意义，疫情之下，Soul这样的回答虽然老套，却真实得生痛。\n年龄越长，分享的欲望也渐趋于无，朋友圈已经没有几个人在分享自己的生活了，彼此抱有互不打扰，或者彼此屏蔽的信条。\n我一直在想，是什么让群里的气氛冷淡了下来，是什么让朋友圈变成了投票微商圈，或者说，是什么让成年人变得沉默，无趣。\n22号灵魂在地球的短暂一天，让我突然意识到成年的真相：平淡，只是因为我们丢失了初生的视角。  Live every minute! \n","date":"2020-12-31T22:49:03+08:00","image":"http://tinohean.top/p/soul/soul_hua5b7a4101002ac3de8aa87e6c8d8ffa6_107809_120x120_fill_q75_box_smart1.JPG","permalink":"http://tinohean.top/p/soul/","title":"Soul"},{"content":"Prometheus常用Exporter及硬件指标 Exporter node_exporter 链接：https://github.com/prometheus/node_exporter\n用途：用于类UNIX内核暴露的系统及硬件指标获取，比如CPU,MEM,DISK,FD等\nprocess_exporter 链接：https://github.com/ncabatoff/process-exporter\n用途：对于一些没有适配Prometheus的应用，可以抓取/proc下面的数据进行指标获取，比如线程数，上下文切换，IO,FD等\nblackbox_exporter 链接：https://github.com/prometheus/blackbox_exporter\n用途：黑盒探针，应用于一些web服务，API服务的可用性监控，可以通过TCP,HTTP,HTTPS等方式，常见指标包括response code, dns lookup time, duration time等\nmysqld_exporter 链接：https://github.com/prometheus/mysqld_exporter\n用途：暴露的mysql监控指标，比如连接数，各类CMD qps，buffer大小等\nredis_exporter 链接：https://github.com/oliver006/redis_exporter\n用途：暴露的redis监控指标，比如QPS，命中率，网络IO，key数量等\ndruid_exporter 链接：https://github.com/wikimedia/operations-software-druid_exporter\n用途：暴露的druid.io相关监控指标，不是很全，可自己定制\njmx_exporter 链接：https://github.com/prometheus/jmx_exporter\n用途：java类应用暴露的相关指标。比如kafka，hadoop生态\n另外，告警体系如果包括钉钉的话，推荐dingtalk_webhook\nAlerts Rules blackbox_exporter 常用的只有存活状态和状态码异常告警\n- alert:BlackboxProbeFailedexpr:probe_success == 0for:0mlabels:severity:criticalannotations:summary:Blackbox probe failed (instance {{ $labels.instance }})description:\u0026#34;Probe failed\\n VALUE = {{ $value }}\\n LABELS = {{ $labels }}\u0026#34;- alert:BlackboxProbeHttpFailureexpr:probe_http_status_code \u0026lt;= 199 OR probe_http_status_code \u0026gt;= 400for:0mlabels:severity:criticalannotations:summary:Blackbox probe HTTP failure (instance {{ $labels.instance }})description:\u0026#34;HTTP status code is not 200-399\\n VALUE = {{ $value }}\\n LABELS = {{ $labels }}\u0026#34;mysql_exporter 通常包括存活状态，慢查询等。\n- alert:MysqlDownexpr:mysql_up == 0for:0mlabels:severity:criticalannotations:summary:MySQL down (instance {{ $labels.instance }})description:\u0026#34;MySQL instance is down on {{ $labels.instance }}\\n VALUE = {{ $value }}\\n LABELS = {{ $labels }}\u0026#34;- alert:MysqlSlowQueriesexpr:increase(mysql_global_status_slow_queries[1m]) \u0026gt; 0for:2mlabels:severity:warningannotations:summary:MySQL slow queries (instance {{ $labels.instance }})description:\u0026#34;MySQL server mysql has some new slow query.\\n VALUE = {{ $value }}\\n LABELS = {{ $labels }}\u0026#34;process_exporter 通常用于进程存活检查\n- alert:ProcessDownexpr:namedprocess_namegroup_num_procs == 0for:1mlabels:severity:highannotations:summary:\u0026#34;Instance {{ $labels.instance }} process is down.\u0026#34;description:\u0026#34;Instance {{ $labels.instance }}: {{ $labels.groupname }} is down\u0026#34;jmx_exporter 这里我只使用了kafka监控\n- name:KafkaStatsAlertrules:- alert:KafkaOfflineAlertexpr:kafka_controller_kafkacontroller_offlinepartitionscount \u0026gt; 0for:5mlabels:severity:averageannotations:summary:\u0026#34;Instance {{ $labels.instance }} have offline partitions.\u0026#34;description:\u0026#34;Offline partitions \u0026gt; 0 (current value: {{ $value }})\u0026#34;- alert:KafkaUnderreplicatedAlertexpr:sum(kafka_cluster_partition_underreplicated) by (instance) \u0026gt; 0for:5mlabels:severity:averageannotations:summary:\u0026#34;Instance {{ $labels.instance }} have partitions underreplicated.\u0026#34;description:\u0026#34;Underreplicated partitions \u0026gt; 0 (current value: {{ $value }})\u0026#34;- alert:KafkaNocontrollerAlertexpr:sum (kafka_controller_kafkacontroller_activecontrollercount ) == 0for:5mlabels:severity:averageannotations:summary:\u0026#34;No active controller.\u0026#34;description:\u0026#34;Current active controller = 0\u0026#34;- alert:KafkaGCtimeAlertexpr:sum without(gc)(rate(jvm_gc_collection_seconds_sum{job=\u0026#34;kafka\u0026#34;}[5m])) \u0026gt; 0.8for:5mlabels:severity:highannotations:summary:\u0026#34;Kafka spent too much time in GC\u0026#34;description:\u0026#34;GCtime spend \u0026gt; 8% (current value: {{ $value }})\u0026#34;node_exporter 以下是筛选后常用的node_exporter获取的硬件指标告警触发语句，包括各种硬件容量预警。\ngroups:- name:hostStatsAlertrules:- alert:hostCpuUsageAlertexpr:sum by(instance) (avg without(cpu) (irate(node_cpu_seconds_total{mode!=\u0026#34;idle\u0026#34;,instance!~\u0026#34;dev.*\u0026#34;}[5m]))) \u0026gt; 0.9for:5mlabels:severity:highannotations:summary:\u0026#34;Instance {{ $labels.instance }} CPU usage high\u0026#34;description:\u0026#34;CPU usage above 90% (current value: {{ $value }})\u0026#34;- alert:hostMemUsageAlertexpr:(node_memory_MemTotal_bytes - node_memory_MemFree_bytes - node_memory_Buffers_bytes - node_memory_Cached_bytes) / node_memory_MemTotal_bytes{cluster!=\u0026#34;develop\u0026#34;} \u0026gt; 0.9for:10mlabels:severity:averageannotations:summary:\u0026#34;Instance {{ $labels.instance }} MEM usage high\u0026#34;description:\u0026#34;MEM usage above 90% (current value: {{ $value }})\u0026#34;- alert:hostDiskUsageAlertexpr:(node_filesystem_size_bytes - node_filesystem_free_bytes ) / node_filesystem_size_bytes {fstype=~\u0026#34;ext[2-5]*\u0026#34;} \u0026gt; 0.8for:5mlabels:severity:highannotations:summary:\u0026#34;Instance {{ $labels.instance }} Disk usage high\u0026#34;description:\u0026#34;Disk usage above 85% (current value: {{ $value }})\u0026#34;- alert:hostDiskInodeUsageAlertexpr:(node_filesystem_files - node_filesystem_files_free)/node_filesystem_files {fstype=~\u0026#34;ext[2-5]*\u0026#34;} \u0026gt; 0.8labels:severity:highannotations:summary:\u0026#34;Instance {{ $labels.instance }} Disk Inode usage high\u0026#34;description:\u0026#34;{{ $labels.instance }} Disk Inode usage above 85% (current value: {{ $value }})\u0026#34;- alert:hostCpuLoadAlertexpr:node_load15 / (count without (cpu, mode) (node_cpu_seconds_total{mode=\u0026#34;system\u0026#34;,cluster!=\u0026#34;develop\u0026#34;})) \u0026gt; 2for:10mlabels:severity:averageannotations:summary:\u0026#34;instance {{ $labels.instance }}) CPU load (15m) high\u0026#34;description:\u0026#34;CPU load (15m) is high\\n VALUE = {{ $value }}\u0026#34;# mute during 0:00-07:00- alert:hostDiskIOAlertexpr:sum by(instance) (avg without(cpu) (irate(node_cpu_seconds_total{mode=~\u0026#34;iowait\u0026#34;,instance!~\u0026#34;dev.*\u0026#34;}[5m]))) \u0026gt; 0.2 and ON() hour() \u0026gt; 0 \u0026lt; 16for:10mlabels:severity:averageannotations:summary:\u0026#34;instance {{ $labels.instance }})Disk I/O is overloaded\u0026#34;description:\u0026#34;{{ $labels.instance }} Disk I/O is overloaded for more than 5 minutes (current value: {{ $value }})\u0026#34;redis_exporter 结合自定义参数获取的指标\n- name:RedisAlertrules:- alert:BlockedClientexpr:redis_blocked_clients \u0026gt; 0 for:1mlabels:severity:highannotations:summary:\u0026#34;redis connection have blocked clients.\u0026#34;description:\u0026#34;redis connections have blocked clients: {{ $value }}.\u0026#34;- alert:RedisClusterSlotsFailexpr:redis_cluster_slots_fail \u0026gt; 0 for:1mlabels:severity:highannotations:summary:\u0026#34;redis cluster have failed slots.\u0026#34;description:\u0026#34;redis cluster have failed slots: {{ $value }}.\u0026#34;- alert:RedisClusterStorageexpr:redis_cluster_stats{type=\u0026#34;StorageUs\u0026#34;} \u0026gt; 85labels:severity:highannotations:summary:\u0026#34;redis cluster storage used over 85%.\u0026#34;description:\u0026#34;redis cluster {{ $labels.ins}} storage used over 85%: {{ $value }}.\u0026#34;- alert:RedisClusterStorageLackexpr:redis_cluster_stats{type=\u0026#34;StorageUs\u0026#34;} \u0026gt; 90labels:severity:disasterannotations:summary:\u0026#34;redis cluster storage used over 90%.\u0026#34;description:\u0026#34;redis cluster {{ $labels.ins}} storage used over 90%: {{ $value }}.\u0026#34;- alert:RedisClusterMaxConnectionexpr:redis_cluster_stats{type=\u0026#34;Connections\u0026#34;} \u0026gt; 5000for:2mlabels:severity:disasterannotations:summary:\u0026#34;redis cluster got too many connections.\u0026#34;description:\u0026#34;redis cluster {{ $labels.ins}} got too many connections : {{ $value }}.\u0026#34;- alert:RedisInstanceDownexpr:redis_up{instance!~\u0026#39;ops-.*\u0026#39;} == 0labels:severity:disasterannotations:summary:\u0026#34;redis instance is down\u0026#34;description:\u0026#34;redis instance {{$labels.alias}} is down\u0026#34;参考文档  https://prometheus.io/docs/instrumenting/exporters/ https://awesome-prometheus-alerts.grep.to/  ","date":"2020-12-23T18:04:34+08:00","image":"http://tinohean.top/p/prometheus%E5%B8%B8%E7%94%A8exporter%E5%8F%8A%E5%91%8A%E8%AD%A6%E8%A7%84%E5%88%99/prometheus_hu29c20fb19d2aaa59a3207076dda60f90_67350_120x120_fill_q75_box_smart1.jpg","permalink":"http://tinohean.top/p/prometheus%E5%B8%B8%E7%94%A8exporter%E5%8F%8A%E5%91%8A%E8%AD%A6%E8%A7%84%E5%88%99/","title":"Prometheus常用Exporter及告警规则"},{"content":"NS游戏清单 已购 已购清单及个人评分\n 超级马里奥:奥德赛 ⭐️⭐️⭐️⭐️⭐️ 马里奥赛车8:豪华版⭐️⭐️⭐️⭐️ 空洞骑士 ⭐️⭐️⭐️⭐️⭐️ 塞尔达传说：旷野之息 ⭐️⭐️⭐️⭐️⭐️ 暗黑破坏神3：永恒之战版 ⭐️⭐️⭐️⭐️ 健身环大冒险 ⭐️⭐️⭐️⭐️ 雷曼：传奇终结版 ⭐️⭐️⭐️⭐️ 血流2 ⭐️⭐️⭐️ 挺进地牢 ⭐️⭐️⭐️ 九张羊皮纸 ⭐️⭐️⭐️  待购 心愿清单\n 茶杯头 巫师3：狂猎 （PC没通关） 路易的鬼屋3：豪华版 （借卡玩通关了，收藏） 超级马里奥：派对 （轰趴没玩够） 死亡细胞 （空洞骑士还不出丝之歌） 歧路旅人 （试玩版很有意思，风格喜欢） 马里奥网球Aces (除了健身环没有其他运动类游戏卡了) ARMS （同上） 塞尔达传说: 旷野之息2 （通关后爬山爬累了） 小小梦魇2：豪华版（DEMO有意思） 蜡烛：火焰的力量 （风格喜欢） 三位一体：终极典藏版 （游戏风格） 光之子：豪华版 （游戏风格） 精灵与萤火意志 （游戏风格） 空洞骑士：丝之歌  ","date":"2020-11-22T21:34:06+08:00","image":"http://tinohean.top/p/nitendo-switch%E6%B8%B8%E6%88%8F%E6%B8%85%E5%8D%95%E4%B8%8D%E5%AE%9A%E6%9C%9F%E6%9B%B4%E6%96%B0/hollow_knight_hue5dd0565eb5a9aa5879d634de1a27c55_568859_120x120_fill_q75_box_smart1.jpeg","permalink":"http://tinohean.top/p/nitendo-switch%E6%B8%B8%E6%88%8F%E6%B8%85%E5%8D%95%E4%B8%8D%E5%AE%9A%E6%9C%9F%E6%9B%B4%E6%96%B0/","title":"Nitendo Switch游戏清单(不定期更新)"},{"content":" 摄于上海青浦 2020年11月8日 下午16:37 \n 摄于上海青浦 2020年11月8日 下午16:44 \n","date":"2020-11-08T20:10:34+08:00","image":"http://tinohean.top/p/%E6%91%84%E5%BD%B1%E8%B6%81%E7%A7%8B%E5%A4%A9%E8%BF%98%E9%80%97%E7%95%99/leaves_hud5c178c04d9776f4d287d9b6fdf28b97_2660847_120x120_fill_q75_box_smart1.JPG","permalink":"http://tinohean.top/p/%E6%91%84%E5%BD%B1%E8%B6%81%E7%A7%8B%E5%A4%A9%E8%BF%98%E9%80%97%E7%95%99/","title":"【摄影】趁秋天还逗留"},{"content":"三十而立 8月5日 台风过境 在家里拉了肚子，疼痛缓解。\n临走的时候早早跟我挥手，我知道她想出去玩。坐上公交车，发现鞋子和裤子湿透，背包湿了一半，眼镜上有零星雨滴。\n卷起了裤腿，就像卷起人生的上半段，一个触目惊心的念头，突然就袭了过来：自己已然走在奔四的路上，时间所剩无几。\n年龄越长，牙齿这座城墙也越加贪婪。随便吃点肉，都会克扣你几缕肉丝，任你牙签牙线各种武装，自岿然不动。\n随之而来的，还有脱离身体航道的脖子。它正在以你闻所未闻的速度跑向衰老。头晕，落枕，酸痛，这些都是它的同伙。每当我试图后仰，咔啪作响的颈椎骨像是冬眠的冰一样裂开，沿着头颅带给你一记沉闷的重击。\n这些崭新的生理体验，是父辈们未能完全传承过的。\n眼前这一垛垛的高楼，淹没了父辈们留下来的路。他们拥有麦地，锄头。我只剩下一双近视昏花的眼睛，视线所及，一切都是模糊的，不可名状的。\n有时候，我不禁会想，如果命运最优解只有一条，双手握住，迎合我的是键盘还是锄头。\n地铁里的灯突然灭了 无数个亮着的手机屏幕如同黑暗宇宙中的小小星球，围绕着同样的脑袋，自转，然后不可避免地走向坍塌，变得冷而硬。\n这些小星球，年轻又衰老，坚强又脆弱。它们和我一样，有着这个年龄段独有的孤独。这种张爱玲所描述的孤独，源于不可卸下的责任，和无可依靠的臂膀。\n中年以后的男人\n时常会觉得孤独\n因为他一睁开眼睛\n周围都是要依靠他的人\n却没有他可以依靠的人\n随着年龄增长的，还有心智。\n认知变得越来越清晰，判断越来越理性。只是理性的行动，换不来情绪的正向。\n这几年，我总是觉得焦虑。梦境里都是消极的情节，灾难，逃生。那些看过的科幻小说，电影，正在通过这样的方式填充着我的梦境。我梦到过移民星球的空气泄露，飞船的坠落，恒星的逼近。而重复次数最多的，无疑是末日版的洪水逃亡，每次我都携家带口，拼命挣扎，到了天明苏醒后，空余疲惫。\n想不清楚的是，这焦虑，是环境给自己的，还是自己潜意识附加的。人，到底是理性的主导还是情绪的奴隶，抑或是两者交替接管的复杂体。\n五一的时候，我带着老婆孩子回了趟黄山，开始了帮家里人接第一单民宿生意。\n接待，清扫，端盘，洗碗，然后在虫子的撞击门窗声中沉沉睡去，一夜无梦。耳鸣和头晕症状消失了。心情大好。找上门来的客人喝酒喝嗨了，拉着老丈人的手一直夸，语气是毫无掩饰的真诚。我在前台坐着，竟也感觉到一种前所未有的平静。不担心房租，房贷，不操心工作，绩效，只需做好最简单的事：招待好客人。\n曾经看过很多隐居的报道，那些放弃城市回归群山的人类，成了城市的谈资。有隐隐的羡慕，内心却是很明确的：这不是我想要的生活。\n上半年，我看了《瓦尔登湖》，试图在里面寻找自己的平静，一无所获。\n实践证明，别人的方式永远不是自己的解药。\n后来，听说终南山因为隐居者众多，房租物价膨胀。那些真的放弃物质的隐居者又被迫回归了城市，令人遗憾。\n世间种种，总会有自我意识的延续载体。而人的站立，终究要在这样的混乱中成立起来，担负起责任，不可推卸，一直前行。\n","date":"2020-08-05T09:25:29+08:00","image":"http://tinohean.top/p/%E4%B8%89%E5%8D%81%E8%80%8C%E7%AB%8B/lonely_hu29c20fb19d2aaa59a3207076dda60f90_70248_120x120_fill_q75_box_smart1.jpg","permalink":"http://tinohean.top/p/%E4%B8%89%E5%8D%81%E8%80%8C%E7%AB%8B/","title":"三十而立"},{"content":"Kubeadm部署kubernetes集群 前期准备 节点规划    主机名 角色 ip 配置 系统版本     k8s-master-01 master 172.21.0.3 4C8G Ubuntu 16.04.1 LTS   k8s-master-02 master 172.21.0.4 4C8G Ubuntu 16.04.1 LTS   k8s-master-03 master 172.21.0.5 4C8G Ubuntu 16.04.1 LTS   k8s-node-01 node 172.21.0.6 4C8G Ubuntu 16.04.1 LTS   k8s-node-02 node 172.21.0.7 4C8G Ubuntu 16.04.1 LTS   k8s-apiserver LB apiserver.k8s.local nginx实现的localproxy     环境配置 所有服务器都需要进行基本环境配置，比如关闭防火墙，关闭swap等\n1 关闭防火墙 ufw disable 2 关闭swap swapoff -a \u0026amp;\u0026amp; sysctl -w vm.swappiness=0 sed -ri \u0026#39;/^[^#]*swap/s@^@#@\u0026#39; /etc/fstab 3 使用chrony进行时间同步 # 配置省略 apt-get install -y chrony 云主机自带ntpd同步机制，可忽略此步骤\n4 安装contained cat \u0026lt;\u0026lt;EOF | sudo tee /etc/modules-load.d/containerd.conf overlay br_netfilter EOF sudo modprobe overlay sudo modprobe br_netfilter # Setup required sysctl params, these persist across reboots. cat \u0026lt;\u0026lt;EOF | sudo tee /etc/sysctl.d/99-kubernetes-cri.conf net.bridge.bridge-nf-call-iptables = 1 net.ipv4.ip_forward = 1 net.bridge.bridge-nf-call-ip6tables = 1 EOF # Apply sysctl params without reboot sudo sysctl --system sudo apt-get update \u0026amp;\u0026amp; sudo apt-get install -y apt-transport-https ca-certificates curl software-properties-common curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key --keyring /etc/apt/trusted.gpg.d/docker.gpg add - sudo add-apt-repository \\  \u0026#34;deb [arch=amd64] https://download.docker.com/linux/ubuntu \\ $(lsb_release -cs)\\ stable\u0026#34; sudo apt-get update \u0026amp;\u0026amp; sudo apt-get install -y containerd.io # Configure containerd sudo mkdir -p /etc/containerd sudo containerd config default | sudo tee /etc/containerd/config.toml # 使用 systemd 作为 cgroup 驱动 sudo vim /etc/containerd/config.toml [plugins.\u0026#34;io.containerd.runtime.v1.linux\u0026#34;] systemd_cgroup = true # 修改配置镜像源为阿里云 [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;.registry.mirrors.\u0026#34;docker.io\u0026#34;] endpoint = [\u0026#34;https://vcw3fe1o.mirror.aliyuncs.com\u0026#34;] [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;] sandbox_image = \u0026#34;https://vcw3fe1o.mirror.aliyuncs.com/pause:3.2\u0026#34; # Restart containerd sudo systemctl restart containerd 4 (可选) 安装docker sudo apt-get update sudo apt-get install \\  apt-transport-https \\  ca-certificates \\  curl \\  gnupg-agent \\  software-properties-common curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - sudo add-apt-repository \\  \u0026#34;deb [arch=amd64] https://download.docker.com/linux/ubuntu \\ $(lsb_release -cs)\\ stable\u0026#34; sudo apt-get update sudo apt-get install docker-ce docker-ce-cli containerd.io 配置docker镜像加速,systemd管理\ncat \u0026gt; /etc/docker/daemon.json \u0026lt;\u0026lt;EOF { \u0026#34;exec-opts\u0026#34;: [\u0026#34;native.cgroupdriver=systemd\u0026#34;], \u0026#34;registry-mirrors\u0026#34;: [ \u0026#34;https://fz5yth0r.mirror.aliyuncs.com\u0026#34;, \u0026#34;http://hub-mirror.c.163.com/\u0026#34;, \u0026#34;https://docker.mirrors.ustc.edu.cn/\u0026#34;, \u0026#34;https://registry.docker-cn.com\u0026#34; ], \u0026#34;storage-driver\u0026#34;: \u0026#34;overlay2\u0026#34;, \u0026#34;storage-opts\u0026#34;: [ \u0026#34;overlay2.override_kernel_check=true\u0026#34; ], \u0026#34;log-driver\u0026#34;: \u0026#34;json-file\u0026#34;, \u0026#34;log-opts\u0026#34;: { \u0026#34;max-size\u0026#34;: \u0026#34;100m\u0026#34;, \u0026#34;max-file\u0026#34;: \u0026#34;3\u0026#34; } } EOF sudo systemctl restart docker.service 5 配置本地host # apiserver指向本机，通过nginx负载 cat \u0026gt;\u0026gt;/etc/hosts \u0026lt;\u0026lt; EOF 172.21.0.3 k8s-master-01 apiserver01.k8s.local # 使用local域名非ip可以方便nginx配置 172.21.0.4 k8s-master-02 apiserver02.k8s.local 172.21.0.5 k8s-master-03 apiserver03.k8s.local 172.21.0.6 k8s-node-01 172.21.0.7 k8s-node-02 127.0.0.1 apiserver.k8s.local #local proxy EOF 6 配置nginx proxy sudo -s mkdir -p /etc/kubernetes cd /etc/kubernetes cat \u0026gt;nginx.conf \u0026lt;\u0026lt; EOF error_log stderr notice; worker_processes 2; worker_rlimit_nofile 130048; worker_shutdown_timeout 10s; events { multi_accept on; use epoll; worker_connections 16384; } stream { upstream kube_apiserver { server apiserver01.k8s.local:6443; server apiserver02.k8s.local:6443; server apiserver03.k8s.local:6443; } server { listen 8443; proxy_pass kube_apiserver; proxy_timeout 10m; proxy_connect_timeout 10s; } } http { aio threads; aio_write on; tcp_nopush on; tcp_nodelay on; keepalive_timeout 75s; keepalive_requests 100; reset_timedout_connection on; server_tokens off; autoindex off; server { listen 8081; location /healthz { access_log off; return 200; } location /stub_status { stub_status on; access_log off; } } } EOF 在containerd中运行nginx proxy\n# 拉取镜像 sudo ctr i pull docker.io/library/nginx:alpine sudo ctr i tag docker.io/library/nginx:alpine nginx # 运行容器 sudo ctr run -d --net-host --null-io --mount type=bind,src=/etc/kubernetes/nginx.conf,dst=/etc/nginx/nginx.conf,options=rbind:ro nginx nginx # 查看进程 sudo ctr task exec --exec-id 0 -t nginx sh 可选（在docker中运行nginx proxy）\nsudo docker run --restart=always \\  -v /etc/kubernetes/nginx.conf:/etc/nginx/nginx.conf \\  -v /etc/localtime:/etc/localtime:ro \\  --name k8s \\  --net host \\  -d \\  nginx:alpine 7 开启内核ipvs模块 kube-proxy使用ipvs会有更好的性能，可伸缩性等优点\nhttps://github.com/kubernetes/kubernetes/blob/master/pkg/proxy/ipvs/README.md\n确保内核模块开启\n ip_vs ip_vs_rr ip_vs_wrr ip_vs_sh nf_conntrack_ipv4  # 安装ipvsadm ipset sudo apt-get install ipvsadm ipset # load module \u0026lt;module_name\u0026gt; sudo modprobe -- ip_vs sudo modprobe -- ip_vs_rr sudo modprobe -- ip_vs_wrr sudo modprobe -- ip_vs_sh sudo modprobe -- nf_conntrack_ipv4 # to check loaded modules, use lsmod | grep -e ip_vs -e nf_conntrack_ipv4 # 永久生效 sudo touch /etc/modules-load.d/ipvs.conf sudo -s cd /etc/modules-load.d echo ip_vs \u0026gt;\u0026gt; ipvs.conf echo ip_vs_rr \u0026gt;\u0026gt; ipvs.conf echo ip_vs_wrr \u0026gt;\u0026gt; ipvs.conf echo ip_vs_sh \u0026gt;\u0026gt; ipvs.conf echo nf_conntrack_ipv4 \u0026gt;\u0026gt; ipvs.conf sudo cat\u0026gt;\u0026gt;/lib/systemd/system/systemd-modules-load.service\u0026lt;\u0026lt;EOF [Install] WantedBy=multi-user.target EOF sudo systemctl daemon-reload sudo systemctl enable --now systemd-modules-load.service # Setup required sysctl params, these persist across reboots. cat \u0026lt;\u0026lt;EOF | sudo tee /etc/sysctl.d/99-kubernetes-ipvs.conf # 修复ipvs模式下长连接timeout问题 小于900即可 net.ipv4.tcp_keepalive_time = 600 net.ipv4.tcp_keepalive_intvl = 30 net.ipv4.tcp_keepalive_probes = 10 EOF # Apply sysctl params without reboot sudo sysctl --system 8 安装kubeadm相关程序 # 国内使用阿里云镜像源进行安装 sudo apt-get update \u0026amp;\u0026amp; sudo apt-get install -y apt-transport-https curl https://mirrors.aliyun.com/kubernetes/apt/doc/apt-key.gpg | sudo apt-key add - cat \u0026lt;\u0026lt;EOF | sudo tee /etc/apt/sources.list.d/kubernetes.list deb https://mirrors.aliyun.com/kubernetes/apt/ kubernetes-$(lsb_release -cs) main EOF sudo apt-get update # 非master节点可以不安装kubectl,集群HA模式下奇数master，建议至少三个 sudo apt-get install -y kubelet kubeadm kubectl sudo apt-mark hold kubelet kubeadm kubectl --- # 国外安装 sudo apt-get update \u0026amp;\u0026amp; sudo apt-get install -y apt-transport-https curl curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add - cat \u0026lt;\u0026lt;EOF | sudo tee /etc/apt/sources.list.d/kubernetes.list deb https://apt.kubernetes.io/ kubernetes-$(lsb_release -cs) main EOF sudo apt-get update sudo apt-get install -y kubelet kubeadm kubectl sudo apt-mark hold kubelet kubeadm kubectl # 可选bash补全 sudo apt-get install bash-completion # 生成自动补全脚本 sudo -s cd /etc/bash_completion.d \u0026amp;\u0026amp; kubectl completion bash \u0026gt;kubectl kubectl配置systemd(通过init配置)\n# /var/lib/kubelet/config.yaml最好 echo \u0026#39;KUBELET_EXTRA_ARGS=\u0026#34;--cgroup-driver=systemd\u0026#34;\u0026#39; \u0026gt;\u0026gt; /etc/default/kubelet # kubeadm init后执行 sudo systemctl daemon-reload sudo systemctl restart kubelet.service kubeadm部署 1. 初始化 master1 配置阿里云镜像，k8s版本，apiserver域名，pod子网掩码等\n# 配置文件 sudo kubeadm config print init-defaults \u0026gt; initconfig.yaml # 修改配置文件后执行检查 sudo kubeadm init --config initconfig.yaml --dry-run sudo kubeadm config images list --config initconfig.yaml # 预先拉取镜像 sudo kubeadm config images pull --config initconfig.yaml sudo kubeadm init --config initconfig.yaml --upload-certs # 非配置文件 sudo kubeadm init \\ --image-repository registry.aliyuncs.com/google_containers \\ --kubernetes-version v1.18.2 \\ --control-plane-endpoint apiserver.k8s.local \\ --apiserver-advertise-address 172.21.0.3 \\ --pod-network-cidr 10.244.0.0/16 \\ --token-ttl 0 配置文件initconfig.yaml\napiVersion:kubeadm.k8s.io/v1beta2kind:InitConfigurationbootstrapTokens:- groups:- system:bootstrappers:kubeadm:default-node-tokentoken:abcdef.0123456789abcdefttl:24h0m0susages:- signing- authenticationlocalAPIEndpoint:#controlPlaneEndpoint是全局apiserver端点，将请求负载到所有HA节点#localAPIEndpoint可以指定本地节点的端点，不配置的话默认是IP#此处可以设置，以防默认获取失败advertiseAddress:172.21.0.3bindPort:6443nodeRegistration:# runtime的socketcriSocket:/run/containerd/containerd.sock#criSocket: /var/run/dockershim.sockname:k8s-master-01taints:- effect:NoSchedulekey:node-role.kubernetes.io/master---apiVersion:kubeadm.k8s.io/v1beta2kind:ClusterConfigurationkubernetesVersion:v1.20.0clusterName:kubernetescertificatesDir:/etc/kubernetes/pki#配置阿里云镜像imageRepository:registry.aliyuncs.com/google_containers#apiserver的LB域名和端口. 单台master可以直接写master的ip：6443controlPlaneEndpoint:apiserver.k8s.local:8443apiServer:timeoutForControlPlane:4m0s# CertSANs sets extra Subject Alternative Names for the API Server signing cert.# HA集群可以配置额外的APIserver证书SAN（ip,dns，域名）certSANs:- 127.0.0.1# 多个master的时候负载均衡出问题了能够快速使用localhost调试- localhost- apiserver.k8s.local# 负载均衡的域名或者vip- 172.21.0.1# master节点1- 172.21.0.2# master节点2- 172.21.0.3# master节点3- apiserver01.k8s.local# master节点1域名- apiserver02.k8s.local# master节点1域名- apiserver03.k8s.local# master节点1域名- master- kubernetes- kubernetes.default- kubernetes.default.svc- kubernetes.default.svc.cluster.localextraArgs:authorization-mode:\u0026#34;Node,RBAC\u0026#34;# admission插件，默认开启可通过`kube-apiserver -h | grep enable-admission-plugins`查看# https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/enable-admission-plugins:\u0026#34;NamespaceLifecycle,LimitRanger,ServiceAccount,PersistentVolumeClaimResize,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota,Priority\u0026#34;# 解决时区问题，挂载模块apiserver,controllermanager,schedulerextraVolumes:- hostPath:/etc/localtimemountPath:/etc/localtimename:localtimereadOnly:truecontrollerManager:extraArgs:cluster-signing-duration:876000h#默认365天，这里配置100年extraVolumes:- hostPath:/etc/localtimemountPath:/etc/localtimename:localtimereadOnly:truedns:type:CoreDNSetcd:local:dataDir:/var/lib/etcdserverCertSANs:# server和peer的localhost,127,::1都默认自带的不需要写- master- 172.21.0.3# master节点1- 172.21.0.4# master节点2- 172.21.0.5# master节点3- etcd01.k8s.local- etcd02.k8s.local- etcd03.k8s.localpeerCertSANs:- master- 172.21.0.3# master节点1- 172.21.0.4# master节点2- 172.21.0.5# master节点3- etcd01.k8s.local- etcd02.k8s.local- etcd03.k8s.local networking:dnsDomain:cluster.localserviceSubnet:10.96.0.0/12#根据网络插件配置，此处为flannel配置podSubnet:10.244.0.0/16scheduler:extraVolumes:- hostPath:/etc/localtimemountPath:/etc/localtimename:localtimereadOnly:true---apiVersion:kubeproxy.config.k8s.io/v1alpha1kind:KubeProxyConfiguration# https://godoc.org/k8s.io/kube-proxy/config/v1alpha1#KubeProxyConfiguration#配置ipvsmode:ipvs ipvs:#ipvs规则刷新周期syncPeriod:15s# 最小刷新周期minSyncPeriod:0s#调度算法scheduler:\u0026#34;rr\u0026#34;excludeCIDRs:null---apiVersion:kubelet.config.k8s.io/v1beta1kind:KubeletConfiguration# https://godoc.org/k8s.io/kubelet/config/v1beta1#KubeletConfigurationserverTLSBootstrap:truecgroupDriver:systemdfailSwapOn:true# 如果开启swap则设置为false初始化后保存相关打印token信息等. 失败后重新init如果报错可以重置\nsudo kubeadm reset 2.配置k8s运行的普通用户 # master1 mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config 3.安装网络插件 # flannel # master1 kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml # 查看pods运行状况 kubectl get pods -n kube-system --- # calico kubectl create -f https://docs.projectcalico.org/manifests/tigera-operator.yaml 4. 其他node加入k8s集群 # node1-2 # 使用 kubeadm init的初始化输出命令加入 kubeadm join apiserver.k8s.local:8443 --token abcdef.0123456789abcdef \\  --discovery-token-ca-cert-hash sha256:71a206b1f412817b352a7c777e09780a3ffa90877fee56206776ad2d396ed498 5. 其他master加入k8s集群 # master1 # （如果init的时候上传了则跳过该步）上传证书，记录后面输出的key sudo kubeadm init phase upload-certs --upload-certs # master2-3  # --certificate-key 指定上述key kubeadm join apiserver.k8s.local:8443 --token abcdef.0123456789abcdef \\  --discovery-token-ca-cert-hash sha256:71a206b1f412817b352a7c777e09780a3ffa90877fee56206776ad2d396ed498 \\  --control-plane --certificate-key 408a84f95681f8b2891ee352d317240604223fc094532b2c9ded966dc8044211 为了便于在任意master节点操作，同样进行用户初始化及host映射\n# master2-3 mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config host映射分别指定本机IP为apiserver.k8s.local\n6. 查看集群nodes # master kubectl get nodes -o wide 输出结果\nNAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME k8s-master-01 Ready control-plane,master 3m15s v1.20.2 172.21.0.3 \u0026lt;none\u0026gt; Ubuntu 16.04.1 LTS 4.4.0-157-generic docker://19.3.14 k8s-master-02 Ready control-plane,master 2m37s v1.20.2 172.21.0.4 \u0026lt;none\u0026gt; Ubuntu 16.04.1 LTS 4.4.0-157-generic docker://19.3.14 k8s-master-03 Ready control-plane,master 2m32s v1.20.2 172.21.0.5 \u0026lt;none\u0026gt; Ubuntu 16.04.1 LTS 4.4.0-157-generic docker://19.3.14 k8s-node-01 Ready \u0026lt;none\u0026gt; 104s v1.20.2 172.21.0.6 \u0026lt;none\u0026gt; Ubuntu 16.04.1 LTS 4.4.0-157-generic docker://19.3.14 k8s-node-02 Ready \u0026lt;none\u0026gt; 110s v1.20.2 172.21.0.7 \u0026lt;none\u0026gt; Ubuntu 16.04.1 LTS 4.4.0-157-generic docker://19.3.14 查看pod\nkubectl -n kube-system get pod -o wide 输出结果\nNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES coredns-7f89b7bc75-7cm78 1/1 Running 0 3m14s 10.244.4.3 k8s-node-01 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; coredns-7f89b7bc75-lb2bz 1/1 Running 0 3m14s 10.244.4.2 k8s-node-01 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; etcd-k8s-master-01 1/1 Running 0 3m23s 172.21.0.3 k8s-master-01 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; etcd-k8s-master-02 1/1 Running 0 2m52s 172.21.0.4 k8s-master-02 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; etcd-k8s-master-03 1/1 Running 0 2m37s 172.21.0.5 k8s-master-03 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-apiserver-k8s-master-01 1/1 Running 0 3m23s 172.21.0.3 k8s-master-01 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-apiserver-k8s-master-02 1/1 Running 0 2m52s 172.21.0.4 k8s-master-02 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-apiserver-k8s-master-03 1/1 Running 0 100s 172.21.0.5 k8s-master-03 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-controller-manager-k8s-master-01 1/1 Running 1 3m23s 172.21.0.3 k8s-master-01 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-controller-manager-k8s-master-02 1/1 Running 0 2m52s 172.21.0.4 k8s-master-02 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-controller-manager-k8s-master-03 1/1 Running 0 103s 172.21.0.5 k8s-master-03 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-flannel-ds-b6lpw 1/1 Running 0 86s 172.21.0.6 k8s-node-01 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-flannel-ds-m945q 1/1 Running 0 86s 172.21.0.4 k8s-master-02 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-flannel-ds-r68bt 1/1 Running 0 86s 172.21.0.7 k8s-node-02 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-flannel-ds-wz97b 1/1 Running 0 86s 172.21.0.5 k8s-master-03 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-flannel-ds-xfwgk 1/1 Running 0 86s 172.21.0.3 k8s-master-01 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-proxy-47f2w 1/1 Running 0 3m15s 172.21.0.3 k8s-master-01 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-proxy-628qj 1/1 Running 0 2m53s 172.21.0.4 k8s-master-02 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-proxy-9hdc4 1/1 Running 0 2m11s 172.21.0.5 k8s-master-03 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-proxy-sqzzk 1/1 Running 0 2m 172.21.0.6 k8s-node-01 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-proxy-z7hj6 1/1 Running 0 2m6s 172.21.0.7 k8s-node-02 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-scheduler-k8s-master-01 1/1 Running 1 3m23s 172.21.0.3 k8s-master-01 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-scheduler-k8s-master-02 1/1 Running 0 2m52s 172.21.0.4 k8s-master-02 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-scheduler-k8s-master-03 1/1 Running 0 96s 172.21.0.5 k8s-master-03 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 参考文档  https://zhangguanzhang.github.io/2019/11/24/kubeadm-base-use/ https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/  ","date":"2020-02-14T22:52:33+08:00","image":"http://tinohean.top/p/kubeadm%E9%83%A8%E7%BD%B2kubernetes%E9%9B%86%E7%BE%A4/kubernetes_hub6bed6b994a2e4811d8630d5db2ab610_722818_120x120_fill_box_smart1_3.png","permalink":"http://tinohean.top/p/kubeadm%E9%83%A8%E7%BD%B2kubernetes%E9%9B%86%E7%BE%A4/","title":"Kubeadm部署kubernetes集群"},{"content":"CMD和ENTRYPOINT的配置比对详解 概述 Dockerfile的语法包括CMD和Entrypoint两种格式都可以配置用于容器内主进程的启动，两者的语义并不是很合适。新上手很容易搞不清楚两者之间的区别和关系，在不同的配置搭配下会产生怎样的运行效果也没有一个条理清晰的思路。\n同样的，k8s也有一套类似的配置语法：command和args。那么k8s的语法中两者又是怎样的搭配关系，它们跟docker语法又是怎样的映射关系呢？\n本文将进行详情分析。\nDocker中CMD和ENTRYPOINT的区别 两种运行模式 不管是CMD还是ENTRYPOINT配置，实际都是有两种模式：SHELL模式，EXEC模式。\nSHELL模式 命令是运行在命令解析器中的，比如linux的/bin/sh -c, windows的cmd /S /C。\n此时容器中的init进程（PID为1）是/bin/sh -c \u0026lt;process\u0026gt;，而不是容器的可执行程序，\u0026lt;process\u0026gt;只做为/bin/sh -c的子进程存在。\nLinux内核机制下，PID为1的进程（通常是init)有区别与其他进程的地方：\n PID为1的进程死掉后，其他所有进程都会被KILL信号杀死（也就是强制退出） 当某父进程死掉后，PID为1的进程会自动继承为其子进程的父进程。 内核不会为PID为1的进程自动注册信号处理程序。PID为1的进程无法接受SIGTERM和SIGINT这类信号，只能SIGKILL强制退出。  因此，我们无法通过docker stop \u0026lt;container\u0026gt;进行优雅退出，因为kubernetes和docker只能发送SIGKILL信号给PID为1的进程，而此时PID为1的/bin/sh -c 没办法传递信号给\u0026lt;process\u0026gt;，只会通过SIGKILL强制退出。而强制退出带来的后果可能是写入中断，数据异常等。\n当然也有解决办法,比如共享进程的namespace，使用专门的init程序，比如tini,supervisor等。最方便的是使用exec命令从shell脚本启动进程，进程会继承PID 1：\n# shell模式下，通过exec配置可以接受signalENTRYPOINT exec cmd1 param1语法如下\n#单独用CMDCMD command param1 param2#单独用ENTRYPOINTENTROYPOINT command param1 param2#搭配使用只会执行ENTRYPOINT，因此CMD此时配置没什么意义以上的运行效果是一样的。\nEXEC模式 主进程就是\u0026lt;process\u0026gt;,所以不经过shell默认的环境变量解释替换过程。由此带来的问题是，类似cd ~和cd $HOME是无效的，因为此时是docker负责进行变量解析。\n这种模式下配置是被解析成json array的，因此必须全部使用双引号！\n语法如下\n# 单独用CMDCMD [\u0026#34;executable\u0026#34;, \u0026#34;param1\u0026#34;, \u0026#34;param2\u0026#34;]# 单独用ENTRYPOINTENTRYPOINT [\u0026#34;executable\u0026#34;, \u0026#34;param1\u0026#34;, \u0026#34;param2\u0026#34;]# 搭配使用1ENTRYPOINT [\u0026#34;executable\u0026#34;]CMD [\u0026#34;param1\u0026#34;, \u0026#34;param2\u0026#34;]# 搭配使用2ENTRYPOINT [\u0026#34;executable\u0026#34;, \u0026#34;param1\u0026#34;]CMD [\u0026#34;param2\u0026#34;]以上语法的运行效果也是一样的。\n配置 简单来说，CMD和ENTRYPOINT的作用都一样，都是用于容器的主进程启动。唯一不同的是，当配置了ENTRYPOINT时，CMD只作为ENTRYPOINT的命令参数存在。两者也可以单独存在。\n因此，配置其实有三种方式\n# exec模式 1. CMD [\u0026#34;exec_cmd\u0026#34;, \u0026#34;param1\u0026#34;] 2. ENTRYPOINT [\u0026#34;exec_cmd\u0026#34;, \u0026#34;param1\u0026#34;] 3. ENTRYPOINT [\u0026#34;exec_cmd\u0026#34;] CMD [\u0026#34;param1\u0026#34;] 当然3也有两种配置方式，比如ENTRYPOINT自带参数和不带参数。\n最佳实践应该是3的方式，即ENTRYPOINT配置可执行命令，CMD搭配默认参数。\n不同配置的运行结果 简单来说，ENTRYPOINT优先级最高，当Dockerfile中配置了ENTRYPOINT并使用EXEC模式时，CMD作为额外参数搭配。\n    No EntryPoint EntryPoint exec_entry p1_entry EntryPoint [\u0026ldquo;exec_entry\u0026rdquo;, \u0026ldquo;p1_entry\u0026rdquo;]     No CMD error, not allowed /bin/sh -c exec_entry p1_entry exec_entry p1_entry   CMD [\u0026ldquo;exec_cmd\u0026rdquo;, \u0026ldquo;p1_cmd\u0026rdquo;] exec_cmd p1_cmd /bin/sh -c exec_entry p1_entry exec_entry p1_entry exec_cmd p1_cmd   CMD [\u0026ldquo;p1_cmd\u0026rdquo;, \u0026ldquo;p2_cmd\u0026rdquo;] p1_cmd p2_cmd /bin/sh -c exec_entry p1_entry exec_entry p1_entry p1_cmd p2_cmd   CMD exec_cmd p1_cmd /bin/sh -c exec_cmd p1_cmd /bin/sh -c exec_entry p1_entry exec_entry p1_entry /bin/sh -c exec_cmd p1_cmd    优先级 整体来说，ENTRYPOINT优先级大于CMD。而同样配置项，通过命令行指定的参数 优先级大于镜像Dockerfile配置，前者可以覆盖掉镜像的配置。\n也就是说: docker run \u0026ndash;entrypoint是最高优先级，ENTRYPOINT是次优先级。\nKubernetes k8s与docker的映射关系 kubernetes中，命令和参数的语义会更合适一些，即command和args, 分别对应Dockerfile中的ENTRYPOINT和CMD。\n详情如下表：\n   Description Docker field name Kubernetes field name     The command run by the container Entrypoint command   The arguments passed to the command Cmd args    配置 与Dockerfile一样，Pod的定义中可以指定容器运行的命令及参数。\napiVersion:v1kind:Podmetadata:name:command-demolabels:purpose:demonstrate-commandspec:containers:- name:command-demo-containerimage:debiancommand:[\u0026#34;printenv\u0026#34;]args:[\u0026#34;HOSTNAME\u0026#34;,\u0026#34;KUBERNETES_PORT\u0026#34;]restartPolicy:OnFailure不同配置的运行结果 简单来说，k8s的优先级高于Dockefile。因此，不同配置的运行结果如下表：\n   Image Entrypoint Image Cmd Container command Container args Command run     [/ep-1] [foo bar]   [ep-1 foo bar]   [/ep-1] [foo bar] [/ep-2]  [ep-2]   [/ep-1] [foo bar]  [zoo boo] [ep-1 zoo boo]   [/ep-1] [foo bar] [/ep-2] [zoo boo] [ep-2 zoo boo]    参考文档  https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/ https://docs.docker.com/engine/reference/builder/ https://github.com/tailhook/vagga/blob/master/docs/pid1mode.rst https://cloud.google.com/architecture/best-practices-for-building-containers  ","date":"2019-09-02T11:47:40+08:00","image":"http://tinohean.top/p/cmd%E5%92%8Centrypoint%E7%9A%84%E9%85%8D%E7%BD%AE%E6%AF%94%E5%AF%B9%E8%AF%A6%E8%A7%A3/cmd_hu6e16d3621c69d4baadb7f1b7e4fbf505_28691_120x120_fill_box_smart1_3.png","permalink":"http://tinohean.top/p/cmd%E5%92%8Centrypoint%E7%9A%84%E9%85%8D%E7%BD%AE%E6%AF%94%E5%AF%B9%E8%AF%A6%E8%A7%A3/","title":"CMD和ENTRYPOINT的配置比对详解"},{"content":"应用容器化改造的设计原则及模式 前言 传统应用在应对流量突发等情况时需要一个完善的紧急响应机制，比如自动弹性伸缩。但由于软硬件限制，自动弹性伸缩在物理机上实现复杂。\n尽管kubernetes学习路线陡峭，复杂度高，但是考虑到成本和效率，推进应用进行容器化改造是一个收益率很高的事情。\n在经历数次微博流量飙升导致深夜紧急手动扩容后，我终于下定决心将两个核心应用迁移到容器上了。\n当然，并不是所有应用都适合容器化，传统应用进行容器化也需要进行一些适配性地改造。\n本文是我调研容器化的改造设计原则及模式的成果。\n设计原则 单一职责原则  SINGLE CONCERN PRINCIPLE \n字面意识就是只做一件事，并把它做好。\n根据Docker最佳实践解释，一个容器应当仅包含一个进程。 此处的进程是指具有唯一父进程且可能拥有多个子进程的单个软件。\n目的是为了增加镜像的可复用性和可移植性，单父进程带来的相同的生命周期和状态也便于kubernetes管理。\n生产环境中不可避免会遇到一些例外。一些复杂的场景需要多个进程协调，此时可以使用边车模式（sidecar)解决。比如tomcat日志归集。\n高可观测性原则  HIGH OBSERVABILITY PRINCIPLE \n容器的设计决定了它的不便观测性，一个运行中的容器对管理者来说是一个黑盒状态，因此不能像虚拟机一样随时TTY登陆查看容器内部的状态，包括进程的日志，进程的启动等。\n那么探针的设计就显得格外重要了。通过探针，kubernetes可以知晓容器的存活，服务就绪状态等。\n除了探针外，我们还需要设计日志接口和监控接口来对接Fluentd,Prometheus等工具进行日常操作，比如日志归集，指标监控等。\n总结下来，容器化的应用需要做好三类接口设计，以便于通过平台进行状态管理和维护：\n 探针 日志 监控  生命周期符合性原则  LIFE-CYCLE CONFORMANCE PRINCIPLE \nkubernetes这类平台为了方便管理容器的生命周期，会产生各种各样的events。这类事件主要是通过Linux信号进行传递,比如SIGTERM和SIGKILL。\n因此，在设计容器应用的时候，开发者需要对这类事件做出恰当的反应逻辑规划并保持符合性。比如进程能接受SIGTERM信号后优雅退出。\n镜像不变性原则  IMAGE IMMUTABILITY PRINCIPLE  镜像作为容器的运行基本,应该保持不变的。这样才有可能进行回滚和滚动发布，从而推动自动化。\n这个原则意味着什么呢？每次变更应该是重新构建一个镜像并应用于所有环境。\n对于一些不同的环境需求，比如开发环境，测试环境，可以通过外部存储runtime数据来进行区分。\n进程可弃型原则  PROCESS DISPOSABILITY PRINCIPLE  基于上面的镜像不变性原则，每次应用变更应该是重新构建，所以容器必须是随时可以销毁的。\n无状态的应用是最适合容器化的，当然有状态的应用也可以通过外部持久卷的方式存储运行时数据来实现这一原则。\n自包含原则  SELF-CONTAINMENT PRINCIPLE  容器作为一个黑盒环境，除了Linux内核外，应该不依赖于任何外部依赖。镜像应该打包了所有应用运行所需的库文件，语言环境等。\n运行时限制原则  RUNTIME CONFINEMENT PRINCIPLE  容器运行时所需的任何硬件资源，比如cpu,mem等，应该做好配额管理，包括使用量声明和实际使用限制。 这个原则主要是为了便于资源调度，弹性伸缩等。\n设计模式 Pod是kubernetes的基本执行单元，封装了应用程序容器、存储资源、唯一网络IP、控制选项等，由单个容器或者多个强耦合共享资源的容器组成。\n这些容器集合共享一个network，特定namespace,volume,已经声明的spec规范，并存于同一个node上\n PID命名空间：Pod中的不同应用程序可以看到其他应用程序的进程ID； 网络命名空间：Pod中的多个容器能够访问同一个IP和端口范围；可以通过localhost通信； IPC命名空间：Pod中的多个容器能够使用SystemV IPC或POSIX消息队列进行通信； UTS命名空间：Pod中的多个容器共享一个主机名； Volumes（共享存储卷）：Pod中的各个容器可以访问在Pod级别定义的Volumes；  Pod中容器是怎么设计安排的，也有一定的模式遵循。\n单容器模式 单容器模式是最简单的一种，遵循上文所述的七种设计原则，开放日志接口，监控接口，生命周期管理。\n单节点多容器模式 有一些普遍存在的场景中，单容器模式由于遵守单一职责原则，无法满足所有需求。此时，可以采用多容器模式，在同一个Pod中运行多个容器，通过共享Volume，命名空间，网络等实现。\nSidecar  Sidecar  边车模式是最为常见的单节点多容器模式。比较常见的一种应用就是web服务附加一个用于日志归集的边车容器，因为共享Volume，所以实现简单。\n除此之外，边车模式还可以作为主程序的配置，服务代理等。\nAmbassador  Ambassador  大使模式类似于一个Proxy,代理了主容器和外部之间的网络通信。好处是开发者只需要关注于核心业务逻辑，不需要操心复杂的网络交互。\n这个模式并不怎么常用。\nAdapter  Adapter  大使模式的意义在于简化与外界的交互。而适配器模式的核心在于把容器内部所有需要与外界交互的接口标准化并暴露出来。常见的使用场景时日志接口，监控接口。\n举个例子，对于监控接口来说，外界可能的方案包括zabbix,prometheus等，适配器模式的功能就是根据不同的方案输出对应格式的数据。\n说到这里可能会存在一些疑惑，为什么开发者不把这些接口的规约定义在程序里？其实，适配器模式主要是为了一些开源程序或者非自研程序设计的，这样开发者就不需要更改源程序进行二次开发。\n多节点模式 分布式的应用需要模块化的设计，多容器之间的彼此协调。\nLeader Election 对于分布式系统来说，最为常见的问题就是选举。这类应用通常需要一个Leader，而其他副本则作为备选和选举者，当Leader节点挂掉后，需要通过其他副本选举出一个新的Leader。\n通过编程实现的选举一般来说逻辑复杂，实现困难。这时候可以把选举逻辑剥离出来成为选举专属容器，由更专业的工程师负责，共同调度。应用开发者只需要关注核心逻辑即可。\nWorker Queue  Worker Queue  传统的队列模式依靠一些成熟的框架，比如hadoop，但也受限于语言环境和框架实现本身。\n容器针对run()和 mount()接口的实现，可以把实现通用的工作队列变得更为简单直接，可以把任何的代码打包成容器，与任意数据构建成一个完整的工作队列系统。\nScatter/Gather  Scatter Gather  分散/收集模式中，根节点接受初始请求，然后把请求分发给大量子节点进行并行计算。每个节点均返回部分数据，根节点把这些数据收集到原始请求的单个相应中。\n实现分散/收集系统，用户需要提供两类容器。首先，一类容器实现叶子节点计算，这个容器执行部分计算并返回相应的结果。 第二种容器是合并容器，这个容器需要汇总所有叶子容器的计算结果，并组织成一个单一的响应后输出。\n容器化改造方法 以下是我针对容器化列出的一些改造点，仅供参考。\n 探针开发\n主要包括健康探针和就绪探针。健康探针用于心跳存活监控，就绪探针用于deployment获取Pod健康状况，ready后再分发流量请求。 目录改造\n主要是去掉了强目录依赖，更新为相对目录。这其实是之前项目的陋习，一直依靠标准化部署手册来维护。还有就是趁机精简了下目录结构。 配置分离\n主要的配置文件通过configmap实现，从主代码库分离。 定时任务\n一些额外的定时任务通过边车模式实现。k8s原生的cronjob无法实现deamonset类的机制，只能实现单节点。可以自定义死循环程序实现。 监控及日志收集\n原fluent-bit和prometheus的各类exporter都已经全部deamonset化,其配置文件也均由configmap托管。 灰度发布\n通过建立多个deployment，匹配不同的标签实现。 启停方式\n容器化后没有reload概念了。每次配置更新都是重建的过程，因此，去除了没意义的生命周期管理脚本。 功能拆分\n原应用依赖定期更新的IP库，本次容器化将IP库解析拆分成微服务，很大程度上精简了镜像大小。  参考文档  https://www.redhat.com/en/resources/cloud-native-container-design-whitepaper https://my.oschina.net/taogang/blog/1809904 https://kubernetes.io/blog/2018/03/principles-of-container-app-design/ https://kubernetes.io/blog/2015/06/the-distributed-system-toolkit-patterns/ https://static.googleusercontent.com/media/research.google.com/zh-CN//pubs/archive/45406.pdf  ","date":"2019-05-15T19:24:26+08:00","image":"http://tinohean.top/p/%E5%BA%94%E7%94%A8%E5%AE%B9%E5%99%A8%E5%8C%96%E6%94%B9%E9%80%A0%E7%9A%84%E8%AE%BE%E8%AE%A1%E5%8E%9F%E5%88%99%E5%8F%8A%E6%A8%A1%E5%BC%8F/container_hued08ec2480e5693b7d58c33103917ef1_43947_120x120_fill_q75_box_smart1.jpeg","permalink":"http://tinohean.top/p/%E5%BA%94%E7%94%A8%E5%AE%B9%E5%99%A8%E5%8C%96%E6%94%B9%E9%80%A0%E7%9A%84%E8%AE%BE%E8%AE%A1%E5%8E%9F%E5%88%99%E5%8F%8A%E6%A8%A1%E5%BC%8F/","title":"应用容器化改造的设计原则及模式"},{"content":"容器构建最佳实践 容器的临时性 这个原则意味着容器可以被随时暂停，销毁，然后重建。容器是无状态的。有状态的数据应该持久化到后端服务中。\n容器构建上下文 build context指docker build执行时的当前目录，默认会找context的Dockerfile进行构建，也可以手动-f指定Dockerfile文件,也可以指定构建目录。\n当前目录的所有目录和文件都会被发送到docker daemon当作构建的上下文，过多、过大的文件都会使得镜像构建时间加长，镜像大小变大。\n因此，build context中应该只包含最少的文件。\n通过标准输出管道构建 build context build context可以是stdin，通过管道传递，这样的好处是不需要传送额外的文件，提升构建速度。\ndocker build -\u0026lt;\u0026lt;EOF FROM busybox RUN echo \u0026#34;hello world\u0026#34; EOF 不过只限于一些简单的构建场景，使用这种方式不可以COPY或者ADD其他文件。\nDockerfile Dockerfile也可以是stdin，但能实现上述的方式更复杂些的操作，可以使用COPY、ADD。\ndocker build -t myimage:latest -f- . \u0026lt;\u0026lt;EOF FROM busybox COPY somefile.txt ./ RUN cat /somefile.txt EOF remote build context 构建上下文可以使用远程地址，适用于一些场景比如git等。\ndocker build -t myimage:latest -f- https://github.com/docker-library/hello-world.git \u0026lt;\u0026lt;EOF FROM busybox COPY hello.c ./ EOF 使用.dockerignore排除文件 同.gitignore类似，docker build也可以通过配置.dockerignore文件排除掉不需要的文件。 格式\n# comment */temp* */*/temp* temp? *.md !readme.md 可以通配符*匹配，！排除。\n多阶段构建 通过缓存机制，可以在最后一步构建阶段创建镜像，实现镜像的最小化。顺序一般是先从很少变动的基础环境层开始到最后的频繁变动层，这样可以更有效使用缓存机制。 顺序如下：\n 安装应用需要的各类工具 安装或更新各种依赖库 构建应用  案例\n# syntax=docker/dockerfile:1FROMgolang:1.16-alpine AS build# Install tools required for project# Run `docker build --no-cache .` to update dependenciesRUN apk add --no-cache gitRUN go get github.com/golang/dep/cmd/dep# List project dependencies with Gopkg.toml and Gopkg.lock# These layers are only re-built when Gopkg files are updatedCOPY Gopkg.lock Gopkg.toml /go/src/project/WORKDIR/go/src/project/# Install library dependenciesRUN dep ensure -vendor-only# Copy the entire project and build it# This layer is rebuilt when a file changes in the project directoryCOPY . /go/src/project/RUN go build -o /bin/project######## 以上都是为了构建项目运行目录而进行的，不需要在实际运行镜像中，因此多阶段构建只需拷贝构建结果到运行时目录即可。# This results in a single layer imageFROMscratchCOPY --from=build /bin/project /bin/projectENTRYPOINT [\u0026#34;/bin/project\u0026#34;]CMD [\u0026#34;--help\u0026#34;]不安装不需要的东西 尽量最小化安装，降低复杂度。构建依赖应该及时清理掉，仅保留运行时依赖。 比如使用 \u0026ndash;no-install-recommends 参数告诉 apt-get 不要安装推荐的软件包 安装完软件包及时清理/var/lib/apt/list/ 缓存 删除中间文件：比如下载的压缩包 删除临时文件：如果命令产生了临时文件，也要及时删除\n案例\n...FROMdebian:9RUN apt-get update \u0026amp;\u0026amp; \\  apt-get install -y \\  [buildpackage] \u0026amp;\u0026amp; \\  [build my app] \u0026amp;\u0026amp; \\  apt-get autoremove --purge \\  -y [buildpackage] \u0026amp;\u0026amp; \\  apt-get -y clean \u0026amp;\u0026amp; \\  rm -rf /var/lib/apt/lists/*单一应用 一个容器应该仅包含一个进程，以便于水平伸缩和容器复用。\n最小化层数 只有RUN,COPY,ADD会创建镜像层，其他的会构建临时镜像，不会增加build的大小。\n因此，应该尽可能少的使用这些命令，比如安装多个软件包，应该通过 \u0026amp;\u0026amp;串起来执行。 除此之外，应该是用多阶段构建的方式减少层数。\n多行参数排序 便于去重和更新，也便于阅读。使用\\进行换行\n案例\nRUN apt-get update \u0026amp;\u0026amp; apt-get install -y \\  bzr \\  cvs \\  git \\  mercurial \\  subversion \\  \u0026amp;\u0026amp; rm -rf /var/lib/apt/lists/*利用构建缓存 image build会根据Dockerfile的顺序执行，在检查每个指令时，Docker会在缓存中查找可以复用的现有镜像，而不是重新创建一个临时镜像。\n基本规则如下\n 从已经存在缓存的父镜像开始，下一条指令与该基础镜像派生的所有子镜像进行比较，查看其中一个是否是使用完全相同的指令构建的，不是的话则缓存无效。 ADD和COPY指令会对每个文件进行校验和检查以确定是否匹配缓存，校验和不考虑文件ctime和mtime。如果校验和不一致，则缓存无效。 除了ADD和COPY之外，容器不会检查文件进行匹配。比如RUN apt-get -y update不会检查更新文件，只会通过命令字符串匹配。  使用尽可能小的基础镜像 基础镜像越小，生成的镜像越小，构建速度越快，比如alpine\n一个完整示例 #FROMdebian:buster-slim######## set -e 用于故障调试，命令结果不为0则直接退出######## set -x 用于问题调试，输出更多明细RUN set -ex \\  \u0026amp;\u0026amp; addgroup --system --gid 101 nginx \\ ######## 创建运行时用户，不可登录，无默认shell,不创建家目录。 \u0026amp;\u0026amp; adduser --system --disabled-login --ingroup nginx --no-create-home --home /nonexistent --gecos \u0026#34;nginx user\u0026#34; --shell /bin/false --uid 101 nginx \\ ######## 构建依赖进行排序 \u0026amp;\u0026amp; buildDeps=\u0026#34;a1 b1 c1\u0026#34; \\  \u0026amp;\u0026amp; apt-get update \\ ####### 使用--no-install-recommends --no-install-suggests来避免安装不必要的软件包 \u0026amp;\u0026amp; apt-get install --no-install-recommends --no-install-suggests -y $buildDeps \\ ######## 使用 apt-mark manual可以将手动安装的软件更新为自动安装的，后续通过purge --auto-remove自动清理不需要的依赖 \u0026amp;\u0026amp; savedAptMark=\u0026#34;$(apt-mark showmanual)\u0026#34; \\  \u0026amp;\u0026amp; apt-mark showmanual | xargs apt-mark auto \u0026gt; /dev/null \\  \u0026amp;\u0026amp; { [ -z \u0026#34;$savedAptMark\u0026#34; ] || apt-mark manual $savedAptMark; } \\ ######## apt-get remove --purge --auto-remove -y 可以清理掉不需要的安装依赖，只保留运行时软件######## 清理apt的安装缓存 rm -rf /var/lib/apt/lists/* \u0026amp;\u0026amp; apt-get remove --purge --auto-remove -y \u0026amp;\u0026amp; rm -rf /var/lib/apt/lists/*COPY docker-entrypoint.sh /ENTRYPOINT [\u0026#34;/docker-entrypoint.sh\u0026#34;]EXPOSE80STOPSIGNALSIGQUITCMD [\u0026#34;nginx\u0026#34;, \u0026#34;-g\u0026#34;, \u0026#34;daemon off;\u0026#34;] 容器运维最佳实践 使用容器的原生日志机制 容器提供了一种简单且标准化的日志处理方式，一般应用可以通过写入stdin和stderr中，使用 docker logs可以查看。在k8s中，可以通过fluent-bit进行收集，进一步转发到其他日志系统中，比如Fluentd,EFK。\n这种标准一般采用json格式，可读性稍差。\n对于一些不好写入stdin和stderr的应用，比如tomcat,可以通过边车模式配置一个专门用于收集日志和处理日志轮询的容器。  sidecar \n确保容器无状态且不可变 无状态并不是不可以有状态，而是任何状态（任何类型的持久性数据）均存储在容器之外，比如持久卷中。\n不变性是指容器在其生命周期内不会被修改。如果应用更新，则需要重建容器。这样容器部署更安全，可重复性更高。\n对于一些外部变量，可以通过configmap或secrets的方式进行装载。\n避免使用特权容器 特权容器可以访问主机所有容器，绕过容器的所有安全功能。使用K8sPSPPrivilegedContainer可以限制特权容器运行。\napiVersion:constraints.gatekeeper.sh/v1beta1kind:K8sPSPPrivilegedContainermetadata:name:psp-privileged-containerspec:match:kinds:- apiGroups:[\u0026#34;\u0026#34;]kinds:[\u0026#34;Pod\u0026#34;]excludedNamespaces:[\u0026#34;kube-system\u0026#34;]使应用易与监控 传统监控类型包括黑盒监控和白盒监控两种。\n黑盒监控是指从外部检查应用的可用性，容器应用和传统应用没有区别。\n白盒监控指使用某种特别访问权限检查应用，并收集终端用户无法查看到的行为指标。一般使用prometheus方案，请求应用的暴露的监控端点，比如/metricsurl。\n有些应用无法实现http端点模式，此类可以通过边车模式，比如java类应用可以通过jmx_exporter边车容器来协助暴露监控数据，再提供给prometheus。  exporter \n公开应用的运行情况 其实就是探针，包括健康探针，就续探针，此处不再赘述。\n避免以root身份运行 主要是因为容器使用的还是主机的内核，存在主机获权的风险。\n谨慎选择镜像版本 使用版本tag,明确版本号。尤为注意的是，尽量不要使用latest，可能由于版本变更大导致无法运行的风险。\nFROM supersoft:1.2.3RUN a-command参考文档  https://cloud.google.com/architecture/best-practices-for-building-containers?hl=zh_cn https://cloud.google.com/architecture/best-practices-for-operating-containers?hl=zh_cn https://docs.docker.com/develop/develop-images/dockerfile_best-practices/  ","date":"2018-08-25T18:06:32+08:00","image":"http://tinohean.top/p/%E5%AE%B9%E5%99%A8%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/best-practice_hu39150d17f5c9712007ceb66088b02be0_125186_120x120_fill_q75_box_smart1.jpeg","permalink":"http://tinohean.top/p/%E5%AE%B9%E5%99%A8%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/","title":"容器最佳实践"},{"content":"监控系统-druid.io 公司的业务日志是通过syslog-ng喂给kafka,提供给spark消费并进行数据清洗，然后把数据导入到druid进行实时聚合查询。\n监控维度覆盖到了druid.io的性能指标上，而针对druid.io的监控能找到的文档不多,官方文档略显简陋而且分散,方案落地的时候碰到了很多坑。\n趁记忆还新鲜整理出这篇文章。\n部署及配置 方案说明 druid配置graphite-emitter,把相关metrics发送给graphite,然后由grafana做前端展示及告警\n版本说明 系统版本：Ubuntu 14.04\ndruid版本：imply-2.5.5 druid-0.12.0\ngraphite版本: latest\ndruid配置 关于监控，druid有几个相关术语简单解释下。\n metrics monitors emitter  metrics  druid提供的用于监控的各类指标，包括各节点的系统资源使用统计，请求相关统计等。\n格式默认是json。通用字段包括 timestamp,metrics,service,host,value。 不过有一些metrics会有额外的dimensions,比如dataSource,type等。\n monitors  同一类别metrics分属的组。\ndruid支持的monitors包括（由于公司架构不包含realtime等节点，这里仅列举我使用的monitors）：\n io.druid.client.cache.CacheMonitor\nHistorical和Broker两个节点的cache统计数据 com.metamx.metrics.SysMonitor\n系统资源使用量统计，这里记得需要下载一个jar包在lib目录 io.druid.server.metrics.HistoricalMetricsMonitor\nHistorical节点数据统计 com.metamx.metrics.JvmMonitor\nJVM相关统计 io.druid.server.metrics.QueryCountStatsMonitor\nBroker,historical两个节点的请求数统计   emitter  发送metrics的工具。 默认是logging,即直接把监控数据写入日志中。后续脚本筛选出来发送给监控系统，扩展性太差。\n还有一种方式\u0026rsquo;http',把metrics通过POST的方式发送给server。 两者可以混合使用，不过日志量实在太大\u0026hellip;除此之外，还有一种选择，就是graphite emitter。\n优点当然是配置灵活，扩展性强。 不过graphite-emitter是社区版的一个插件，需要独立安装。\n 这里需要重点注意的是:\n 不同的metrics归属于不同的monitors,而不同的节点支持的monitors不同\n 所以monitors配置尽量不要配置在common下，以免出现报错，甚至出现druid节点无法启动的现象。\n比如监控historical的统计数据，就需要开启io.druid.server.metrics.HistoricalMetricsMonitor，如果配置在common中，你会发现其他节点有各种诡异的报错，各种启动失败\u0026hellip;跳坑无数的血泪教训\n部署过程 安装graphite 关于graphite，只需要了解大概架构即可。其实有点类似ELK的模式，graphite也包括三种模块。\n carbon: 接收数据。本身并不会采集数据。这一点官方文档特意强调了。 whisper: 时序数据库，存储carbon接收的数据。替代品包括influxdb等。 graphite-web: Django编写的一个web应用，把数据渲染成图形。替代品当然是美丽优雅的grafana了。  如上所述，其实graphite本身也有绘图功能，只不过效果相当简陋\u0026hellip;还好grafana有官方app可以直接使用graphite当作data source。\n尝试了一次graphite的部署过程，实在痛苦不堪。模块多，python及pip多版本的兼容性是个很大的坑，依赖库django的兼容性更是坑上加坑。我花了一天的时间调试才勉强搭建好测试环境。\n此后线上环境为了节省时间，我直接上了docker。\nsudo apt-get update sudo apt-get install \\  linux-image-extra-$(uname -r) \\  linux-image-extra-virtual sudo apt-get install \\  apt-transport-https \\  ca-certificates \\  curl \\  software-properties-common curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - sudo add-apt-repository \\  \u0026#34;deb [arch=amd64] https://download.docker.com/linux/ubuntu \\ $(lsb_release -cs)\\ stable\u0026#34; sudo apt-get update sudo apt-get install docker-ce 国内docker pull也是蜜汁缓慢，为此我还得配置一个加速器\nsudo vim /etc/default/docker DOCKER_OPTS=\u0026#34;--registry-mirror=https://registry.docker-cn.com\u0026#34; sudo service docker restart 终于可以愉快运行了\nsudo docker run -d --name graphite --restart=always -p 80:80 -p 2003-2004:2003-2004 -p 2023-2024:2023-2024 -p 8125:8125/udp -p 8126:8126 graphiteapp/graphite-statsd 此处端口可以看到一共开放了7个端口，其中，80端口是前端web页面。\ncarbon支持两种协议的接收方式，一种是The plaintext protocol，端口为2003，另一种是The pickle protocol，端口为2004。\n而2023和2024分别对应两个接收方式的聚合器carbon-aggregator.py的端口。\n8125和8126是statsd对应的端口，此处用不到。\n此处需要注意的一点是，nc和telnet等命令使用的是The plaintext protocol协议，也就是2003端口。而druid的graphite-emitter使用的是The pickle protocal,即2004端口。\n测试数据\nPORT=2003 SERVER=graphite.your.org echo \u0026#34;local.random.diceroll 4 `date +%s`\u0026#34; | nc -c ${SERVER} ${PORT} 访问http://127.0.0.1可以看到略显丑陋的界面。\ndruid-graphiter插件部署 druid插件包括core和community两种，前者集成在druid里面，无需额外按照，后者需要根据匹配的版本号去自行安装。\n安装包命名格式如下：\n名字 ：版本号 io.druid.extensions.contrib:graphite-emitter:0.12.0 安装graphite-emitter extensions\ncd $Druid_PATH/ java \\  -cp \u0026#34;lib/*\u0026#34; \\  -Ddruid.extensions.directory=\u0026#34;extensions\u0026#34; \\  -Ddruid.extensions.hadoopDependenciesDir=\u0026#34;hadoop-dependencies\u0026#34; \\  io.druid.cli.Main tools pull-deps \\  --no-default-hadoop \\  -c \u0026#34;io.druid.extensions.contrib:graphite-emitter:0.12.0\u0026#34; 配置include Emitter vim common.runtime.properties druid.extensions.loadList=[...,\u0026quot;graphite-emitter\u0026quot;] #最后面加入graphite-emitter即可 配置Monitoring 此处记得大坑，不同的monitors一定要针对性配置在对应的模块下，不要全部写入common。\nvim common.runtime.properties druid.monitoring.monitors=[\u0026quot;com.metamx.metrics.SysMonitor\u0026quot;,\u0026quot;com.metamx.metrics.JvmMonitor\u0026quot;] druid.emitter=graphite druid.emitter.graphite.hostname=127.0.0.1 druid.emitter.graphite.port=2004 #记得上文解释的两种协议吧。这里配置pickle协议的2004端口 druid.emitter.graphite.eventConverter={\u0026quot;type\u0026quot;:\u0026quot;whiteList\u0026quot;, \u0026quot;namespacePrefix\u0026quot;: \u0026quot;druid.SYS_JVM\u0026quot;, \u0026quot;ignoreHostname\u0026quot;:true, \u0026quot;ignoreServiceName\u0026quot;:false, \u0026quot;mapPath\u0026quot;:\u0026quot;conf/druid_metric_whitelist.j son\u0026quot;} druid.emitter.logging.logLevel=info druid.monitoring.emissionPeriod=PT1m #自定义设置采集周期。默认1min vim broker/runtime.properties #monitor druid.monitoring.monitors=[\u0026quot;io.druid.client.cache.CacheMonitor\u0026quot;,\u0026quot;io.druid.server.metrics.QueryCountStatsMonitor\u0026quot;] druid.emitter=graphite druid.emitter.graphite.hostname=127.0.0.1 druid.emitter.graphite.port=2004 druid.emitter.graphite.eventConverter={\u0026quot;type\u0026quot;:\u0026quot;whiteList\u0026quot;, \u0026quot;namespacePrefix\u0026quot;: \u0026quot;druid.broker\u0026quot;, \u0026quot;ignoreHostname\u0026quot;:true, \u0026quot;ignoreServiceName\u0026quot;:false, \u0026quot;mapPath\u0026quot;:\u0026quot;conf/druid_metric_whitelist.js on\u0026quot;} druid.emitter.logging.logLevel=info druid.monitoring.emissionPeriod=PT1m vim historical/runtime.properties #monitor druid.monitoring.monitors=[\u0026quot;io.druid.server.metrics.HistoricalMetricsMonitor\u0026quot;,\u0026quot;io.druid.client.cache.CacheMonitor\u0026quot;,\u0026quot;io.druid.server.metrics.QueryCountStatsMonitor\u0026quot;] druid.emitter=graphite druid.emitter.graphite.hostname=127.0.0.1 druid.emitter.graphite.port=2004 druid.emitter.graphite.eventConverter={\u0026quot;type\u0026quot;:\u0026quot;whiteList\u0026quot;, \u0026quot;namespacePrefix\u0026quot;: \u0026quot;druid.historical\u0026quot;, \u0026quot;ignoreHostname\u0026quot;:true, \u0026quot;ignoreServiceName\u0026quot;:false, \u0026quot;mapPath\u0026quot;:\u0026quot;conf/druid_metric_whitelis t.json\u0026quot;} druid.emitter.logging.logLevel=info druid.monitoring.emissionPeriod=PT1m 关于all 和白名单模式 all会接收到很多无用的信息。白名单模式只需配置自己监控的metrics即可。\n关于白名单json文件，根据官方文档关于metrics的接收按需配置，格式请参考https://github.com/druid-io/druid/blob/master/extensions-contrib/graphite-emitter/src/main/resources/defaultWhiteListMap.json\nconf/druid_metric_whitelist.json { \u0026#34;query/time\u0026#34;: [ \u0026#34;dataSource\u0026#34;, \u0026#34;type\u0026#34; ], \u0026#34;query/node/time\u0026#34;: [ \u0026#34;dataSource\u0026#34;, \u0026#34;type\u0026#34; ], \u0026#34;query/success/count\u0026#34;: [], \u0026#34;query/failed/count\u0026#34;: [], \u0026#34;query/interrupted/count\u0026#34;: [], \u0026#34;query/segment/time\u0026#34;: [ \u0026#34;dataSource\u0026#34;, \u0026#34;type\u0026#34; ], \u0026#34;query/cache/delta\u0026#34;: [], \u0026#34;query/cache/total\u0026#34;: [], \u0026#34;segment/max\u0026#34;: [], \u0026#34;segment/used\u0026#34;: [], \u0026#34;segment/usedPercent\u0026#34;: [], \u0026#34;segment/count\u0026#34;: [], \u0026#34;jvm/gc\u0026#34;: [\u0026#34;gcName\u0026#34;], \u0026#34;jvm/mem\u0026#34;: [\u0026#34;memKind\u0026#34;], \u0026#34;jvm/pool\u0026#34;: [\u0026#34;poolKind\u0026#34;,\u0026#34;poolName\u0026#34;], \u0026#34;sys/net\u0026#34;:[], \u0026#34;sys/fs\u0026#34;:[], \u0026#34;sys/mem\u0026#34;:[], \u0026#34;sys/cpu\u0026#34;:[] } namespacePrefix此时可以根据节点分别命名为\ndruid.broker druid.historical ... 这样graphite查询结构就可以节点分开了\n文档参考 关于监控metrics的说明 http://druid.io/docs/0.12.0/operations/metrics.html\n关于monitors及metrics配置相关的说明 http://druid.io/docs/latest/configuration/index.html\n关于graphite-emitter配置相关的说明 http://druid.io/docs/0.12.0/development/extensions-contrib/graphite.html\n","date":"2017-10-16T22:43:41Z","image":"http://tinohean.top/p/%E7%9B%91%E6%8E%A7%E6%96%B9%E6%A1%88-druid%E7%AF%87/druid_hu29d20a37b17955b30e24040a85cde0bd_50267_120x120_fill_q75_box_smart1.jpeg","permalink":"http://tinohean.top/p/%E7%9B%91%E6%8E%A7%E6%96%B9%E6%A1%88-druid%E7%AF%87/","title":"监控方案-druid篇"},{"content":"kafka监控篇 方案调研 针对kakfa的监控，简单调研了下，业界比较流行的几款解决方案:\n KafkaOffsetMonitor Kakfa Manager kakfa Monitor Kafka Eagle  对于当前的zabbix+grafana监控框架来说，都略显笨重。\n而kafka官方文档关于monitor的说明：可以通过内嵌的JMX的方式获取kafka各类metrics,无需安装任何额外的组件。同时zabbix可以通过zabbix-java-gateway的组件实现监控JMX。\n考虑再三，决定了技术方案：\n 开启kafka的JMX zabbix server通过zabbix-java-gateway访问JMX获取各metrics grafana实现监控数据可视化  优点:\n当然是跟现有的监控体系结合在一起，不用再额外搭建一套新的监控系统了。\n缺点:\n经过一系列跳坑，发现kafka的java跟zabbix-java-gateway以及zabbix-server跟zabbix-java-gateway之间的版本兼容性不太好，实测不向下兼容。鉴于此，kakfa与zabbix-server的系统版本最好一致。\n针对这个缺点，可以通过JMX的command line工具,脚本化获取需要监控的metrics,然后定时任务发送给zabbix,无需安装zabbix-java-gateway。\n此类工具很多，最简单的是一个jar包。\n配置部署 zabbix server配置了java-gateway(192.168.1.10)后会pre-fork与配置数量相匹配的java-poller进程，java-poller会请求JavaGateway，而JavaGateway会调用JMX management API，获取已对其开放权限的JMX(192.169.1.11)的各类metrics值\nzabbix-server 新建host,监控接口选择JMX，默认端口是12345。\n一个典型的JMX item包含两个方面：\njmx[object_name,attribute_name] object_name就是选择的kafka的metrics,然后后面就是该metrics对应的attributes 监控的各类metrics。此处最好做一个模板,方便后续的导入\nzabbix-server.conf相关配置\n#javaGateway的IP JavaGateway=192.168.1.10 #默认端口10052 JavaGatewayPort=10052 #javaPoller的进程数量 StartJavaPollers=5 zabbix-java-gateway sudo apt-get install zabbix-java-gateway 默认配置即可\nkafka JMX JMX的调用需要在启动脚本加入相关参数配置，指定端口及放权IP,这里仅列举下不需要ssl认证的简单配置\nlocal:\n-Dcom.sun.management.jmxremote \\ -Dcom.sun.management.jmxremote.port=12345 \\ -Dcom.sun.management.jmxremote.authenticate=false \\ -Dcom.sun.management.jmxremote.ssl=false \\ remote:\n-Dcom.sun.management.jmxremote \\ -Dcom.sun.management.jmxremote.port=12345 \\ -Dcom.sun.management.jmxremote.authenticate=false \\ -Dcom.sun.management.jmxremote.ssl=false \\ -Djava.rmi.server.hostname=192.168.1.10 我们先看下kafka启动脚本kafka-server-start.sh调用的kafka-run-class.sh的相关参数.\n# JMX settings if [ -z \u0026#34;$KAFKA_JMX_OPTS\u0026#34; ]; then KAFKA_JMX_OPTS=\u0026#34;-Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false \u0026#34; fi # JMX port to use if [ $JMX_PORT ]; then KAFKA_JMX_OPTS=\u0026#34;$KAFKA_JMX_OPTS-Dcom.sun.management.jmxremote.port=$JMX_PORT\u0026#34; fi # JVM performance options if [ -z \u0026#34;$KAFKA_JVM_PERFORMANCE_OPTS\u0026#34; ]; then KAFKA_JVM_PERFORMANCE_OPTS=\u0026#34;-server -XX:+UseG1GC -XX:MaxGCPauseMillis=20 -XX:InitiatingHeapOccupancyPercent=35 -XX:+DisableExplicitGC -Djava.awt.headless=true\u0026#34; fi 可以看到默认配置除了端口号和remote ip这两个都齐全了。 所以最简单的local启动：\nJMX_PORT=12345 ./zookeeper-server-start.sh ../config/zookeeper.properties 如果是remote访问，直接修改kafka-run-class.sh的配置：\n# JVM performance options if [ -z \u0026#34;$KAFKA_JVM_PERFORMANCE_OPTS\u0026#34; ]; then KAFKA_JVM_PERFORMANCE_OPTS=\u0026#34;-server -XX:+UseG1GC -XX:MaxGCPauseMillis=20 -XX:InitiatingHeapOccupancyPercent=35 -XX:+DisableExplicitGC -Djava.awt.headless=true -Dcom. sun.management.jmxremote.port=12345 -Djava.rmi.server.hostname=192.168.1.10\u0026#34; fi 为了保持配置清晰，建议封装在一个脚本内执行，先export相关变量配置。\n关于metrics 上文提及的jar包可以通过命令行的方式获取当前kakfa版本所有支持的metrics\njava -jar cmdline-jmxclient-0.10.3.jar - 192.168.1.11:12345 | grep kakfa kafka.server:name=FailedProduceRequestsPerSec,topic=cunToDruid,type=BrokerTopicMetrics kafka.server:delayedOperation=Rebalance,name=NumDelayedOperations,type=DelayedOperationPurgatory kafka.log:name=Size,partition=1,topic=__consumer_offsets,type=Log kafka.log:name=LogEndOffset,partition=32,topic=__consumer_offsets,type=Log kafka.network:name=ResponseQueueTimeMs,request=ApiVersions,type=RequestMetrics kafka.network:name=RemoteTimeMs,request=UpdateMetadata,type=RequestMetrics kafka.log:name=LogEndOffset,partition=3,topic=__consumer_offsets,type=Log kafka.network:name=ResponseQueueSize,type=RequestChannel kafka.server:name=BytesRejectedPerSec,topic=cun,type=BrokerTopicMetrics ... 每个metrics都有若干个attributes\njava -jar cmdline-jmxclient-0.10.3.jar - 192.168.1.11:12345 kafka.network:name=RequestsPerSec,request=Produce,type=RequestMetrics Attributes: Count: Attribute exposed for management (type=long) RateUnit: Attribute exposed for management (type=java.util.concurrent.TimeUnit) MeanRate: Attribute exposed for management (type=double) OneMinuteRate: Attribute exposed for management (type=double) FiveMinuteRate: Attribute exposed for management (type=double) FifteenMinuteRate: Attribute exposed for management (type=double) EventType: Attribute exposed for management (type=java.lang.String) Operations: objectName: Operation exposed for management Parameters 0, return type=javax.management.ObjectName 当前获取的是produce的QPS,可以看到有多个属性值可以选择，我们这边选择OneMinuteRate\njava -jar cmdline-jmxclient-0.10.3.jar - 192.168.1.11:12345 kafka.network:name=RequestsPerSec,request=Produce,type=RequestMetrics OneMinuteRate 获取结果如下\n07/10/2017 11:45:36 +0800 org.archive.jmx.Client OneMinuteRate: 1158.9830099452165 其它推荐关注的metrics可见官方文档\n最终效果 结合grafana,做了监控可视化处理。 kafka \n","date":"2017-07-11T22:45:20Z","image":"http://tinohean.top/p/%E7%9B%91%E6%8E%A7%E6%96%B9%E6%A1%88-kafka%E7%AF%87/grafana_hud6a20f3436e4696f01e55623167f1a17_283099_120x120_fill_box_smart1_3.png","permalink":"http://tinohean.top/p/%E7%9B%91%E6%8E%A7%E6%96%B9%E6%A1%88-kafka%E7%AF%87/","title":"监控方案-kafka篇"},{"content":"需求场景 有一个python上线部署脚本，兼具重启和健康检查功能，当上线重启的时候希望定时任务的健康检查不要触发任何重启操作。\n同时，如果执行重启的时候碰到正在例行的健康检查，等待，直到健康检查结束。\n即: 当一个脚本被多次调用时（非多线程），使用flock控制其内同一个函数的调用控制。\n由于是多进程，线程锁是无效的，简单的使用python的fcntl模块的文件锁可以实现这个需求\n关于文件锁 linux内核提供的一种进程之间资源防竞争机制。防止多进程间使用同一个共享资源时同时操作造成错乱。\n鉴于linux中一切皆文件，所以文件锁有很多使用的空间。\n锁分为劝告锁和强制锁。劝告锁只是一个非强制的约定规则，即可以不遵守。所以需要不同进程间约定协调。而强制锁则由内核进行强制约束。\n此外，两种锁都可以有共享和排他的分类，分别是共享锁（读锁）和排他锁（写锁）\n 共享锁:\n我在它身上上了一把锁，你也可以过来读取它 排他锁:\n我在它身上上了一把锁，我要写东西进去，这期间你不能读也不能写  关于两种锁的不同进程间的兼容关系\n     进程B共享锁 进程B排他锁     进程A锁     无 是 是   共享锁 是 否   排他锁 否 否    强制锁由内核进行强制约束，当文件加有共享锁后，其他进程对文件对写操作会被内核阻止，当文件加有排他锁后，其他进程任何操作都会被阻止塞。阻止包括是堵塞-等待，非堵塞-立刻返回错误信号EAGAIN。\n详细如下表\n   当前锁 堵塞读 堵塞写 非堵塞读 非堵塞写     共享锁 正常读 堵塞 正常读 EAGAIN   排他锁 堵塞 堵塞 EAGAIN EAGAIN    注意：\n 一个进程可以对同一文件同时上共享锁和排他锁 劝告锁只有劝告作用，需要进程间自我约束协调 强制锁后的阻止包括堵塞和非堵塞，堵塞会持续等待直到锁解除，非堵塞则会直接返回错误信号并退出  shell的flock命令 shell提供了flock命令实现文件锁。flock只可以添加劝告锁，并且只能针对整个文件进行加锁，无法对文件的部分进行加锁。\nflock对强制锁的实现 进程A调用了flock对文件t.txt加LOCK.EX的前提下，进程B对t.txt的访问情况\n   进程B 访问情况     直接访问t.txt 可以访问   先调用flock加锁，再访问t.txt 不可访问    也就是说，具体使用的时候不同进程必须先调用flock加锁实现强制锁的功能。\nflock的参数\nUsage: flock [-sxun][-w #] fd# #第一种模式直接对文件描述符加锁，解锁。其实当程序执行完毕，文件描述符自动关闭后锁也会自动施放 flock [-sxon][-w #] file [-c] command... #第二种模式先对文件加锁，然后执行命令 flock [-sxon][-w #] directory [-c] command... #第三种模式先对目录加锁，然后执行命令 Options: -s --shared Get a shared lock # 共享锁 -x --exclusive Get an exclusive lock #排他锁 -u --unlock Remove a lock # 解锁 -n --nonblock Fail rather than wait # 非堵塞，直接报错 -w --timeout Wait for a limited amount of time #堵塞，等待时间 -o --close Close file descriptor before running command #第二/三种模式下先关闭文件描述符，再执行命令 -c --command Run a single command string through the shell #加锁后执行的shell命令 -h --help Display this text #显示帮助 -V --version Display version # 显示版本信息 两种使用例子：\nshell scripts:\n#!/bin/bash #/opt/test.sh ( flock -xn 110 \u0026amp;\u0026amp; echo success || echo failed ; ) 110\u0026gt;/tmp.lock crontab：\n* * * * * flock -xn /tmp/crontab.lock -c 'bash /opt/test.sh' 前者封装在脚本里需要判断flock的执行结果$?,然后根据执行结果判断下一步操作。\ncrontab中可以利用flock的-c参数，加锁成功才会执行-c后面的cmd。\npython的fcntl模块 fcntl模块的flock()跟shell的flock类似，是对unix系统flock()的封装。只是无法实现等待时间(flock -w)的操作。堵塞状态下会一直等待下去，非堵塞状态会抛出OSError异常。\nlockf()是对unix系统fcntl()的封装，不仅仅能实现对文件整体的lock，还能实现对指定文件位置的lock。\n看下文档中对于flock()几种锁的介绍\nman 2 flock FLOCK(2) Linux Programmer's Manual FLOCK(2) NAME flock - apply or remove an advisory lock on an open file SYNOPSIS #include \u0026lt;sys/file.h\u0026gt; int flock(int fd, int operation); DESCRIPTION Apply or remove an advisory lock on the open file specified by fd. The argument operation is one of the following: LOCK_SH Place a shared lock. More than one process may hold a shared lock for a given file at a given time. LOCK_EX Place an exclusive lock. Only one process may hold an exclusive lock for a given file at a given time. LOCK_UN Remove an existing lock held by this process. A call to flock() may block if an incompatible lock is held by another process. To make a nonblocking request, include LOCK_NB (by ORing) with any of the above operations. A single file may not simultaneously have both shared and exclusive locks. 其中提及三种锁操作，LOCK_SH，LOCK_EX，LOCK_UN，分别是共享锁，排他锁，解锁。同时LOCK_NB是非堵塞机制，可以与共享锁或者排他锁共存配置\n与shell类似，python使用文件锁可以很容易实现对文件的控制。文章前面需求背景中实现对脚本上线和健康检查的两个使用场景下重启方法的调用控制就是通过文件锁的方式实现的。\n示例伪代码\n#!/usr/bin/env python # _*_ coding: utf-8 _*_ import sys from subprocess import call import fcntl ... def do_stop_start(tag): if tag == \u0026#34;kill\u0026#34;: do_cmd = kill_cmd else: do_cmd = start_cmd for cmd in do_cmd: call(cmd,shell=True) ... if __name__ == \u0026#34;__main__\u0026#34;: #get FD of lock_file deploy_file = open(\u0026#34;/tmp/.lock\u0026#34;,\u0026#34;r\u0026#34;) if sys.argv[1] == \u0026#34;health\u0026#34;: fcntl.flock(deploy_file,fcntl.LOCK_EX|fcntl.LOCK_NB) ... do_stop_start(\u0026#34;start\u0026#34;) else: fcntl.flock(deploy_file,fcntl.LOCK_EX) ... do_stop_start(\u0026#34;stop\u0026#34;) do_stop_start(\u0026#39;start\u0026#39;) ... fcntl.flock(deploy_file,fcntl.LOCK_UN) deploy_file.close() 这样脚本执行上线部署的时候，会尝试添加一个排他锁，而如果此前已经有健康检查执行的话，文件已经加了排他锁，脚本会直接进入堵塞状态，等待健康检查完成，再进行进程重启操作。\n同样，如果健康检查执行之前，已经有上线部署，此时健康检查想给文件添加一个排他锁和非堵塞锁，由于此前上线部署已经加了排他锁，上锁失败，同时是非堵塞锁，直接抛出异常退出。\n 参考文档\nhttps://docs.python.org/3/library/fcntl.html#module-fcntl\nhttps://www.ibm.com/developerworks/cn/linux/l-cn-filelock/\n","date":"2017-04-13T21:01:45+08:00","image":"http://tinohean.top/p/%E6%96%87%E4%BB%B6%E9%94%81flock%E5%AE%9E%E7%8E%B0%E5%87%BD%E6%95%B0%E7%BA%A7%E5%88%AB%E7%9A%84%E8%B0%83%E7%94%A8%E6%8E%A7%E5%88%B6/flock_huaea571c15354d96edc7109bd2ba7d9dc_422973_120x120_fill_q75_box_smart1.jpeg","permalink":"http://tinohean.top/p/%E6%96%87%E4%BB%B6%E9%94%81flock%E5%AE%9E%E7%8E%B0%E5%87%BD%E6%95%B0%E7%BA%A7%E5%88%AB%E7%9A%84%E8%B0%83%E7%94%A8%E6%8E%A7%E5%88%B6/","title":"文件锁flock实现函数级别的调用控制"},{"content":"雪山 上个月我在公司内部做了一次关于docker的技术分享，主题就是简单介绍下docker的技术原理及使用。在做PPT的时候，我发现这并不是一个那么容易的事情。\n难点其一是技术原理的理解。与研发不同的是，运维平时并不注重深入了解技术实现这一块。对于工具型的技术方案，一般仅限于了解其大概的模块框架，并不具备对外者讲解清楚技术封装的基本能力。这样的学习模式好处是上手快，翻翻文档很容易就部署一套完整的体系。坏处也显而易见，遇到系统性的问题很难有思路去排查，对于配置也是照本宣科，依靠百度和谷歌解决大部分问题。而对于传统运维来讲，系统底层技术面的匮乏，加上代码阅读能力较低，能够深入了解一个技术框架的实现并不是一件很轻松的事情。\n难点其二是分享本身。分享之所以成为分享，其核心的逻辑是先自我消化，然后再通过讲解的方式阐述消化的内容。这一吞吐的过程势必暴露分享者的技术储备。同时，讲解者必须具备一定的逻辑和分析能力 ，将脑中形成的技术面归纳通过文图的形式条理化，具象化。将复杂的事情简单讲解出来是一个需要持久化练习的能力。人类本身讨厌一切复杂的事情。我更喜欢类比这个学习方法。类比的过程可以将抽象的层面具象化，更容易理解和记忆。然而技术上的理解，很难有类比的对象。我只能尝试用概括的方式把消化掉的一堆堆的文档和技术文章精炼为几个简单的图表。\n分享是一个自我思考及重复记忆的过程。在准备技术原理的过程中，我推翻了很多之前的技术误解，也消除了一些模棱两可的问题，对于docker的整体认知更加全面。除此之外，这一次分享把脑中的技术记忆渗入了更为底层的维度。而学习正是从理解到重复记忆最后到实践的过程。\n我的短板在系统化，基础。学的太杂而不深入，大部分技术都是浅尝辄止。可能也是很多运维的通病吧。写作可以强迫我去渗透，总结，记录。\n突然想起高中时读过的海子的一首诗 《最后一夜和第一日的献诗》\n今夜你的黑头发\n是岩石上寂寞的黑夜\n牧羊人用雪白的羊群\n填满飞机场周同的黑暗\n黑夜比我更早睡去\n黑夜是神的伤口\n你是我的伤口\n羊群和花朵也是岩石的伤口\n雪山 用大雪填满飞机场周围的黑暗\n雪山女神吃的是野兽穿的是鲜花\n今夜九十九座雪山高出天堂\n使我彻夜难眠\n在当年词汇匮乏的我心里留下了史诗般的具象。雪山这一画面感极强的描述简直惊为天人。这就是诗歌的魅力，也是海子的天才，简单的几个词语的使用，就能形成如此诗化的意境。海子心中的雪山真实的欲求是什么，我们已经无从知道了。然而，仅仅从意象而言，诗歌本身就有了意义。把脑中的画面形象地描述出来，写作者追求的终极目的之一无非是这样的境界了吧。为此，我决定开设个人网站，把内心所想化作一座座雪山。\n或许哪天，你看到这些文字，知晓了我当时的心境。那么，你也曾抵达我的峰顶。\n","date":"2017-04-02T18:16:30+08:00","image":"http://tinohean.top/p/%E9%9B%AA%E5%B1%B1/snowmountain_huc3a92b42c0177518a8a9285045a91360_599460_120x120_fill_box_smart1_3.png","permalink":"http://tinohean.top/p/%E9%9B%AA%E5%B1%B1/","title":"雪山"}]